{"/":{"title":"Devkraft Technologies","content":"\n# [[notes/Generative_AI.md | Generative AI]]","lastmodified":"2023-08-25T15:19:54.874738913Z","tags":[]},"/notes/20230628030800-Checklist":{"title":"Checklist","content":"\n\u003c!-- \ntags: \n--\u003e\n\n\u003c!--internal\nparent:: [[]]\nchild:: [[]]\nrelated:: [[]]\n--\u003e\n\n\u003c!--external\n- [ ] []()\n--\u003e\n\n# Checklist\n\n\u003e what would be genAI first experience would be for users in an ideal scenario. \n\n- [ ] How [[notes/20230628030901 Generative AI|Generative AI]] works\n- [ ] APIs of OpenAI\n- [ ] Azure versions of [[notes/20230630042526 ChatGPT|ChatGPT]]\n- [ ] How to send customer data to such APIs\n- [ ] How to to keep customer data private\n- [ ] [[notes/20230703043154 Vector Database|Vector Database]]/[[notes/20230703031649 Word Embedding|Embedding]]\n- [ ] How to improve accuracy and reduce hallucinations\n- [ ] How to serve structured data back to customer in consistent manner\n- [ ] (optional) Other LLMs\n- [ ] (optional) Chaining results from different LLMs\n\t- [ ] [[notes/20230703061520 LangChain|LangChain]]\n- [ ] How to experiment with and provide this to users at low cost\n","lastmodified":"2023-08-25T15:19:54.874738913Z","tags":[""]},"/notes/20230628030801-Links":{"title":"Links","content":"\n\u003c!-- \ntags: \n--\u003e\n\n\u003c!--internal\nparent:: [[]]\nchild:: [[]]\nrelated:: [[]]\n--\u003e\n\n\u003c!--external\n- []()\n--\u003e\n\n# Links\n\n\n- [ ] [Vinija Jain](https://vinija.ai/)\n  - [ ] [models](https://vinija.ai/models/)\n- [ ] [Neural Networks: Zero to Hero](https://karpathy.ai/zero-to-hero.html)\n- [ ] [Awesome-LLM](https://github.com/Hannibal046/Awesome-LLM)\n- [ ] [Awesome-LLM-Large-Language-Models-Notes ](https://github.com/kyaiooiayk/Awesome-LLM-Large-Language-Models-Notes)\n- [ ] [BLOOM Is the Most Important AI Model of the Decade](https://thealgorithmicbridge.substack.com/p/bloom-is-the-most-important-ai-model)\n- [ ] [What Is ChatGPT Doing … and Why Does It Work?](https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/)\n- [ ] [The Practical Guides for Large Language Models ](https://github.com/Mooler0410/LLMsPracticalGuide)\n- [ ] [Learn Large Language Models ](https://gist.github.com/rain-1/eebd5e5eb2784feecf450324e3341c8d)\n- [ ] [LLaMA - Run LLM in A Single 4GB GPU ](https://github.com/juncongmoo/pyllama)\n- [ ] [if you are curious about large language models](http://www.iasylum.net/writings/2023-03-29-if-you-are-curious-about-LLMs.html)\n- [ ] [alpaca](https://github.com/tatsu-lab/stanford_alpaca)\n  - [ ] [AlpacaDataCleaned](https://github.com/gururise/AlpacaDataCleaned)\n- [ ] [llama](https://github.com/facebookresearch/llama) \n- [ ] [dalai](https://github.com/cocktailpeanut/dalai)\n- [ ] [EdgeGPT](https://github.com/acheong08/EdgeGPT)\n- [ ] [GPT4all](https://github.com/nomic-ai/gpt4all)\n- [ ] [Open-Assistant](https://github.com/LAION-AI/Open-Assistant)\n- [ ] [ChatGPT](https://github.com/acheong08/ChatGPT)\n- [ ] [web-llm](https://github.com/mlc-ai/web-llm)\n- [ ] [Data Ingestion for Large Language Models](https://blog.apify.com/what-is-data-ingestion-for-large-language-models/)\n- [ ] [Generating LLM embeddings with open source models in PostgresML](https://postgresml.org/blog/generating-llm-embeddings-with-open-source-models-in-postgresml)\n- [ ] [sweep.dev](https://sweep.dev/?trk=feed-detail_main-feed-card-text)\n  - [ ] sends all code to openAI ChatGPT\n- [ ] [Vertex AI](https://cloud.google.com/vertex-ai)\n\n## Shared\n\n\n- [https://github.com/gradio-app/gradio](https://github.com/gradio-app/gradio)\n- [https://huggingface.co/spaces](https://huggingface.co/spaces)\n- [https://huggingface.co/spaces/THUDM/CogVideo](https://huggingface.co/spaces/THUDM/CogVideo)\n- [https://filmora.wondershare.com/ai/alternative-to-synthesia-for-ai-video-generation.html?gclid=Cj0KCQjwk96lBhDHARIsAEKO4xYtIMxfYk4zp4nCTpB5sCBmuShbdSGzN5Q0HTsEfEUQxsjgbbIFFbgaAjD1EALw_wcB](https://filmora.wondershare.com/ai/alternative-to-synthesia-for-ai-video-generation.html?gclid=Cj0KCQjwk96lBhDHARIsAEKO4xYtIMxfYk4zp4nCTpB5sCBmuShbdSGzN5Q0HTsEfEUQxsjgbbIFFbgaAjD1EALw_wcB)\n- [https://playgroundai.com/create?](https://playgroundai.com/create?)\n- [https://poe.com/ChatGPT](https://poe.com/ChatGPT)\n- [https://www.perplexity.ai/](https://www.perplexity.ai/)\n- [https://chat.forefront.ai/](https://chat.forefront.ai/)\n- [https://docs.synthesia.io/reference/auth](https://docs.synthesia.io/reference/auth)\n- [https://elevenlabs.io/pricing](https://elevenlabs.io/pricing)\n- [https://github.com/AUTOMATIC1111/stable-diffusion-webui](https://github.com/AUTOMATIC1111/stable-diffusion-webui)\n- [https://www.reddit.com/r/StableDiffusion/comments/134yxoc/any_free_alternatives_to_midjourney/](https://www.reddit.com/r/StableDiffusion/comments/134yxoc/any_free_alternatives_to_midjourney/)\n- [https://github.com/DevkraftTechnologies/langchain](https://github.com/DevkraftTechnologies/langchain)\n- [https://www.eraser.io/home](https://www.eraser.io/home)\n- [https://www.happyaccidents.ai/](https://www.happyaccidents.ai/)\n- [https://robocorp.com/portal/robot/ekipalen/Assistant-OpenAI-test](https://robocorp.com/portal/robot/ekipalen/Assistant-OpenAI-test)\n- [https://supabase.com/docs/guides/ai/examples/image-search-openai-clip](https://supabase.com/docs/guides/ai/examples/image-search-openai-clip)\n- [https://supabase.com/docs/guides/cli](https://supabase.com/docs/guides/cli)\n- [https://fastlane.is](https://fastlane.is)\n- [https://www.futurepedia.io/](https://www.futurepedia.io/)\n- [https://www.pulumi.com/ai/](https://www.pulumi.com/ai/)\n- [https://invideo.io/](https://invideo.io/)\n- [https://www.heygen.com/](https://www.heygen.com/)\n- [https://videocandy.com/](https://videocandy.com/)\n- [https://app.runwayml.com/](https://app.runwayml.com/)\n- [https://briapi2.redoc.ly/#section/Introduction](https://briapi2.redoc.ly/#section/Introduction)\n","lastmodified":"2023-08-25T15:19:54.874738913Z","tags":[""]},"/notes/20230628030810-Large-Language-Models-LLMs":{"title":"Large Language Models (LLMs)","content":"\n\u003c!-- \ntags: \n--\u003e\n\n\u003c!--internal\nparent:: [[]]\nchild:: [[]]\nrelated:: [[]]\n--\u003e\n\n\u003c!--external\n- []()\n--\u003e\n\n# Large Language Model (LLMs)\n\nThe parameters a like a model's memory, the more parameters a model has the more sophisticated task it can perform\n## Usecases\n\n- Chatbots\n- Writing\n- Text Summarisation\n- Translation\n\t- Languages for ex. English to German\n\t- Natural Language to Programming Language\n- Information Retrieval\n\t- Named Entity Recognition (NER)\n- Augmenting LLMs\n\t- Connect to external data sources like APIs, database etc\n\n## Limitations\n\nThere are some fundamental limitations of LLMs that can be difficult to overcome through training alone \n- tendencies to invent information when it doesn't know an answer, or\n- limited ability to carry out complex reasoning and mathematics","lastmodified":"2023-08-25T15:19:54.874738913Z","tags":[""]},"/notes/20230628030901-Generative-AI":{"title":"Generative AI","content":"\n\u003c!--\ntags:\n--\u003e\n\n\u003c!--internal\nparent:: [[]]\nchild:: [[]]\nrelated:: [[]]\n--\u003e\n\n\u003c!--external\n\n- []()\n--\u003e\n\n# Generative AI\n\nGenerative AI refers to Artificial Intelligence models that generates novel data, information, or documents\n","lastmodified":"2023-08-25T15:19:54.874738913Z","tags":[""]},"/notes/20230628030911-Build-GPT-from-scratch":{"title":"Build GPT from scratch","content":"\n\u003c!--\ntags: \n--\u003e\n\n\u003c!--internal\nparent:: [[notes/20230628031147 Generative Pretrained Transformer (GPT)|GPT]]\nchild:: [[]]\nrelated:: [[]]\n--\u003e\n\n\u003c!--external\n- []()\n--\u003e\n\n# Build GPT from scratch\n\n- [ ] watch andrej karpathy builds [[notes/20230628031147 Generative Pretrained Transformer (GPT)|GPT]] from Scratch in his [video](https://www.youtube.com/watch?v=kCc8FmEb1nY)","lastmodified":"2023-08-25T15:19:54.874738913Z","tags":[""]},"/notes/20230628031147-Generative-Pretrained-Transformer-GPT":{"title":"Generative Pretrained Transformer (GPT)","content":"\n\u003c!-- \ntags: \n--\u003e\n\n\u003c!--internal\nparent:: [[notes/20230628030901 Generative AI|Generative AI]]\nchild:: [[]]\nrelated:: [[]]\n--\u003e\n\n\u003c!--external\n- []()\n--\u003e\n\n# Generative Pretrained Transformer (GPT)\n\nGPT stands for Generative Pretrained [[notes/20230703011428 Transformers|Transformer]]\n","lastmodified":"2023-08-25T15:19:54.874738913Z","tags":[""]},"/notes/20230628031546-Generative-Adversarial-Networks-GANs":{"title":"Generative Adversarial Networks (GANs)","content":"\n\u003c!-- \ntags: \n--\u003e\n\n\u003c!--internal\nparent:: [[]]\nchild:: [[]]\nrelated:: [[]]\n--\u003e\n\n\u003c!--external\n- []()\n--\u003e\n\n# Generative Adversarial Networks (GANs)\n\nGenerative Adversarial Networks or GANs are a type of neural network that uses two competing networks - a generator and a discriminator\n\n- The generator creates fake outputs, and \n- The discriminator tries to tell the difference between the fake outputs and real-world data\n\nThrough this back-and-forth process, the GAN is able to produce outputs that are indistinguishable from real data\n","lastmodified":"2023-08-25T15:19:54.874738913Z","tags":[""]},"/notes/20230628032202-How-does-GPT-work":{"title":"How does GPT work","content":"\n\u003c!-- \ntags: \n--\u003e\n\n\u003c!--internal\nparent:: [[notes/20230628031147 Generative Pretrained Transformer (GPT)|GPT]]\nchild:: [[]]\nrelated:: [[]]\n--\u003e\n\n\u003c!--external\n- [ ] [How does GPT Obtain its Ability? Tracing Emergent Abilities of Language Models to their Sources](https://www.notion.so/How-does-GPT-Obtain-its-Ability-Tracing-Emergent-Abilities-of-Language-Models-to-their-Sources-b9a57ac0fcf74f30a1ab9e3e36fa1dc1)\n--\u003e\n\n# How does [[notes/20230628031147 Generative Pretrained Transformer (GPT)|GPT]] work\n\n","lastmodified":"2023-08-25T15:19:54.874738913Z","tags":[""]},"/notes/20230628033256-GPT-3":{"title":"GPT-3","content":"\n\u003c!-- \ntags: \n--\u003e\n\n\u003c!--internal\nparent:: [[notes/20230628031147 Generative Pretrained Transformer (GPT)|GPT]]\nchild:: [[]]\nrelated:: [[]]\n--\u003e\n\n\u003c!--external\n- [ ] [How GPT3 Works - Visualizations and Animations](https://jalammar.github.io/how-gpt3-works-visualizations-animations/)\n--\u003e\n\n# GPT-3\n\n- The architecture is a transformer decoder model\n- [[notes/20230628031147 Generative Pretrained Transformer (GPT)|GPT]]-3 works is novel due to it's huge scale. \n- GPT-3 is MASSIVE. \n- It encodes what it learns from training in 175 billion numbers (called parameters). \n- These numbers are used to calculate which token to generate at each run.\n- The untrained model starts with random parameters. \n- Training finds values that lead to better predictions\n- GPT-3 generates output one token at a time\n\n\n","lastmodified":"2023-08-25T15:19:54.874738913Z","tags":[""]},"/notes/20230630020251-Discriminative-AI":{"title":"Discriminative AI","content":"\n\u003c!--\ntags:\n--\u003e\n\n\u003c!--internal\nparent:: [[]]\nchild:: [[]]\nrelated:: [[]]\n--\u003e\n\n\u003c!--external\n\n- []()\n--\u003e\n\n# Discriminative AI\n\nDiscriminative AI is useful when we want to make some sort of **decision**\n\nFor example, we might want to predict whether someone is at risk for cancer given some biometric data - height, weight, blood pressure, etc.\n\n\u003c!-- \n![[_attachments_/Pasted image 20230630020702.png]]\n--\u003e\n\nInstead of a list of numbers as above, we might  have an image.\n\n\u003c!-- \n![[_attachments_/Pasted image 20230630020713.png]]\n--\u003e\n\nNot all Discriminative AI techniques model a [[notes/20230630021015 Conditional Distribution|Conditional Distribution]] because not all Discriminative AI methods even model a distribution in the first place.\n\nFor example, **Support Vector Machines** are **NOT** **probabilistic**, but SVM are still used for Discriminative AI\n","lastmodified":"2023-08-25T15:19:54.874738913Z","tags":[""]},"/notes/20230630020526-Generative-AI-vs-Discriminative-AI":{"title":"Generative AI vs Discriminative AI","content":"\n\u003c!-- \ntags: \n--\u003e\n\n\u003c!--internal\nparent:: [[]]\nchild:: [[]]\nrelated:: [[]]\n--\u003e\n\n\u003c!--external\n- []()\n--\u003e\n\n# Generative AI vs Discriminative AI\n\nWhen working with [[notes/20230630020251 Discriminative AI|Discriminative AI]], we don’t care about these features in and of themselves - we only care about them insofar as they help us make a decision.\n\nIn contrast, with [[notes/20230628030901 Generative AI|Generative AI]], we do care about these features themselves. \n\nIndeed, the whole goal of Generative AI is to understand how these features relate in order to generate plausible data. \n\nFor example, suppose our goal is to generate a representative sample of humans in terms of body size (considering only height and weight here for simplicity). \n\n\u003c!--\n![[notes/images/Pasted image 20230630020929.png]]\n--\u003e\n\nIn particular, it is unrealistic to have someone that tall and thin, or that short and wide; and it’s even less likely to have a sample of 3 such extremes at the same time.\n\nInstead, we need to model the statistical distribution of weight and height in the population we wish to sample from, in order to generate more realistic novel data\n\n\u003c!--\n![[notes/images/Pasted image 20230630020945.png]]\n--\u003e\n","lastmodified":"2023-08-25T15:19:54.874738913Z","tags":[""]},"/notes/20230630020526-Generative-AI-vs-Discriminative-AI-A-mathematical-perspective":{"title":"Generative AI vs Discriminative AI - A mathematical perspective","content":"\n\u003c!--\ntags:\n--\u003e\n\n\u003c!--internal\nparent:: [[notes/20230630020526 Generative AI vs Discriminative AI]]\nchild:: [[]]\nrelated:: [[]]\n--\u003e\n\n\u003c!--external\n\n- []()\n--\u003e\n\n# Generative AI vs Discriminative AI - A Mathematical Perspective\n\nDiscriminative AI is considered to be modeling a **[[notes/20230630021015 Conditional Distribution|Conditional Distribution]]** whereas Generative AI is considered to be modeling a **[[notes/20230630021029 Joint Distribution|Joint Distribution]]**\n\n![[notes/images/Pasted image 20230630035154.png]]\n","lastmodified":"2023-08-25T15:19:54.874738913Z","tags":["rn"]},"/notes/20230630021015-Conditional-Distribution":{"title":"Conditional Distribution","content":"\n\u003c!-- \ntags: \n--\u003e\n\n\u003c!--internal\nparent:: [[]]\nchild:: [[]]\nrelated:: [[]]\n--\u003e\n\n\u003c!--external\n- []()\n--\u003e\n\n# Conditional Distribution\n\nA conditional distribution provides the probability of A occurring given B\n\nFor example, we can ask what the probability of rolling a specific number on a 6 sided die is. \n\nFor a fair die, the probability will be ⅙ for all numbers on the die. \n\nNow, consider how these probabilities change when we are given extra information. What if we are told that the number we rolled was even? \n\nWith this information, the probabilities the number being a 1, 3, or 5 are now zero, and the probabilities of rolling a 2, 4, or 6 are now ⅓.\n","lastmodified":"2023-08-25T15:19:54.874738913Z","tags":[""]},"/notes/20230630021029-Joint-Distribution":{"title":"Joint Distribution","content":"\n\u003c!-- \ntags: \n--\u003e\n\n\u003c!--internal\nparent:: [[]]\nchild:: [[]]\nrelated:: [[]]\n--\u003e\n\n\u003c!--external\n- []()\n--\u003e\n\n# Joint Distribution\n\nA joint distribution provides the probability of A occurring alongside B\n\nFor example, what is the probability of rolling a 2 on a first die roll and a 3 on a second die roll? \n\nIn this case, probability is 1/36 assuming, again, that the die is fair.\n","lastmodified":"2023-08-25T15:19:54.874738913Z","tags":[""]},"/notes/20230630041814-Generative-AI-A-Mathematical-Perspective":{"title":"Generative AI - A Mathematical Perspective","content":"\n\u003c!-- \ntags: \n--\u003e\n\n\u003c!--internal\nparent:: [[notes/20230628030901 Generative AI|Generative AI]]\nchild:: [[]]\nrelated:: [[]]\n--\u003e\n\n\u003c!--external\n- []()\n--\u003e\n\n#  Generative AI - A Mathematical Perspective\n\nGenerative AI is modeling a [[notes/20230630021029 Joint Distribution|Joint Distribution]] because the distribution itself is the object of interest\n\nonce we model the distribution, we can use it for different outcomes\n\n- we can perform density estimates for example estimating the probabilities of someone being taller than 71 in (180 cm) and lighter than 150 lbs (68 kg)\n- we can sample from this distribution to generate novel data, which we can do for various reasons.\n  - use the generated data to train another AI model.\n  - use the generated data in its own right as we do with models like DALL-E 2","lastmodified":"2023-08-25T15:19:54.874738913Z","tags":["rn"]},"/notes/20230630042526-ChatGPT":{"title":"ChatGPT","content":"\n\u003c!-- \ntags: \n--\u003e\n\n\u003c!--internal\nparent:: [[notes/20230628031147 Generative Pretrained Transformer (GPT)|GPT]]\nchild:: [[]]\nrelated:: [[]]\n--\u003e\n\n\u003c!--external\n- [ ] [How ChatGPT actually works](https://www.assemblyai.com/blog/how-chatgpt-actually-works/)\n--\u003e\n\n# ChatGPT\n\nChatGPT uses [[notes/20230628031546 Generative Adversarial Networks (GANs)|GANs]] to generate responses to input text, allowing it to engage in natural-sounding conversations with humans","lastmodified":"2023-08-25T15:19:54.874738913Z","tags":[""]},"/notes/20230630042902-How-to-use-Generative-AI":{"title":"How to use Generative AI","content":"\n\u003c!-- \ntags: \n--\u003e\n\n\u003c!--internal\nparent:: [[]]\nchild:: [[]]\nrelated:: [[]]\n--\u003e\n\n\u003c!--external\n- []()\n--\u003e\n\n# How to use Generative AI\n\nThe most general way to think about [[notes/20230628030901 Generative AI|Generative AI]] is as a mapping from (potential) antecedents to desired consequents\n\nLet’s say that the **goal** is to become the preeminent brand in the X market. \n\nA Y% increase in shares of  roduct X on social media is the **measurable outcome** that serves as a good **proxy** for **measuring progress** towards our **goal**.\n\n![[notes/images/Pasted image 20230630043715.png]]\n\nwhat we are doing is seeking to **implement some change or idea** (an antecedent) that will lead to the **desired outcome** (the consequent).\n\n![[notes/images/Pasted image 20230630043923.png]]\n\nwe don’t _know_ if an idea is  an antecedent. That is, will the implementation of this idea actually lead to the desired consequent? \n\nTherefore it is our job to investigate, implement, and iterate on multiple potential antecedents in an attempt to observe the desired consequent\n\nWe must establish, to the best of our ability, that an idea is in fact an antecedent to the desired consequent - i.e. that the change **entails** the consequent\n\n![[notes/images/Pasted image 20230702204208.png]]\n\n\nThe critical point here is that **the implementation details of the potential antecedents are the bottleneck in this process**. \n\nIt is straightforward to think of what outcome we _want_ and to think of what change _might lead_ to that result\n\nbut the details of _how_ we bridge the two is the challenge. \n\nRegardless of the specific domain, the **human implementation details** are where the bulk of the work is to be done and where the bottleneck occurs.\n\n![[notes/images/Pasted image 20230702204251.png]]\n\nGenerative AI can be thought of **as a tool to build the bridge between the potential antecedents and the desired consequent**.\n\n![[notes/images/Pasted image 20230702204452.png]]\n\nwe can use Generative AI in order to **expedite implementation details** involved with putting these ideas into action\n\nfor ex. use Generative AI to [[notes/20230702210325 Add a button to product page to share the product|add a button to product page to share the product]]","lastmodified":"2023-08-25T15:19:54.874738913Z","tags":[""]},"/notes/20230702204625-Modern-Generative-AI":{"title":"Modern Generative AI","content":"\n\u003c!-- \ntags: \n--\u003e\n\n\u003c!--internal\nparent:: [[]]\nchild:: [[]]\nrelated:: [[]]\n--\u003e\n\n\u003c!--external\n- []()\n--\u003e\n\n# Modern Generative AI\n\n[[notes/20230628030901 Generative AI|Generative AI]] is not a new tech, but the recent explosion in performance and interest can be attributed to advances made in the last 5 years\n\nIn the image space (models like [DALL-E 2](https://www.assemblyai.com/blog/how-dall-e-2-actually-works/), [Imagen](https://www.assemblyai.com/blog/how-imagen-actually-works/), [Stable Diffusion](https://www.assemblyai.com/blog/stable-diffusion-1-vs-2-what-you-need-to-know/), etc.), advances have relied on the development of [Diffusion Models](https://www.assemblyai.com/blog/diffusion-models-for-machine-learning-introduction/).\n\nIn the language space (models like [ChatGPT](https://www.assemblyai.com/blog/how-chatgpt-actually-works/), GPT-4, etc.), advances have been made by the [scaling](https://www.assemblyai.com/blog/emergent-abilities-of-large-language-models/) of the [Transformer](https://www.youtube.com/watch?v=_UVfwBqcnbM\u0026ref=assemblyai.com) architecture.","lastmodified":"2023-08-25T15:19:54.874738913Z","tags":["rn"]},"/notes/20230702210325-Add-a-button-to-product-page-to-share-the-product":{"title":"Add a button to product page to share the product","content":"\n\u003c!-- \ntags: \n--\u003e\n\n\u003c!--internal\nparent:: [[]]\nchild:: [[]]\nrelated:: [[]]\n--\u003e\n\n\u003c!--external\n- []()\n--\u003e\n\n# Add a button to product page to share the product\n\nFrom a technical standpoint, implementing this change (i.e. adding this button) can take a bit of time\n\n- we might have to design a component that matches the website\n- we might have to incorporate it into a template so it shows up on multiple pages \n- we have to explore how the APIs of external social media platforms work\n- we might have to require login first or account creation, which would require interacting with a database\n\nwe can use Generative AI to write the (straightforward) code to make this happen \n\nAsk ChatGPT to write a simple Next.js component that implements these social share buttons\n\n```\nwrite a Next.js component that contains buttons to share the current page on social media\n```\n\n![[notes/images/Pasted image 20230702212152.png]]\n\nThe main human work that would need to be done is review. \n\nNote that the model even gives an explanation of the code which would make the review time even quicker\n\nwe can go further and ask the model to add style to our component using some specific tools in our stack\n\n![[notes/images/Pasted image 20230702212238.png]]\n\n```js\nimport { useRouter } from 'next/router';\nimport { makeStyles } from '@material-ui/core/styles';\nimport Button from '@material-ui/core/Button';\n\nconst useStyles = makeStyles((theme) =\u003e ({\n  button: {\n    margin: theme.spacing(1),\n    boxShadow: 'none',\n    '\u0026:hover': {\n      boxShadow: theme.shadows[4],\n    },\n  },\n}));\n\nconst ShareButtons = () =\u003e {\n  const router = useRouter();\n  const pageUrl = `https://yourwebsite.com${router.asPath}`;\n  const classes = useStyles();\n\n  const shareOnTwitter = () =\u003e {\n    window.open(`https://twitter.com/intent/tweet?url=${pageUrl}`);\n  };\n\n  const shareOnFacebook = () =\u003e {\n    window.open(`https://www.facebook.com/sharer/sharer.php?u=${pageUrl}`);\n  };\n\n  const shareOnLinkedIn = () =\u003e {\n    window.open(`https://www.linkedin.com/sharing/share-offsite/?url=${pageUrl}`);\n  };\n\n  return (\n    \u003cdiv\u003e\n      \u003cButton variant=\"contained\" color=\"primary\" className={classes.button} onClick={shareOnTwitter}\u003e\n        Share on Twitter\n      \u003c/Button\u003e\n      \u003cButton variant=\"contained\" color=\"primary\" className={classes.button} onClick={shareOnFacebook}\u003e\n        Share on Facebook\n      \u003c/Button\u003e\n      \u003cButton variant=\"contained\" color=\"primary\" className={classes.button} onClick={shareOnLinkedIn}\u003e\n        Share on LinkedIn\n      \u003c/Button\u003e\n    \u003c/div\u003e\n  );\n};\n\nexport default ShareButtons;\n```\n\nwith solid domain knowledge and good prompt engineering principles, a human can use Generative AI to truncate the time required to implement such a feature\n\nThis is a simple example to communicate the essential idea outlined above. The actual task is not complicated, but **that’s the point**.\n\nA lot of the work that leads to valuable business outcomes is not complicated, and Generative AI can be used to expedite the implementation of these changes\n\nThe bottom line is that **generative AI makes it fast and easier to implement ideas**","lastmodified":"2023-08-25T15:19:54.874738913Z","tags":[""]},"/notes/20230703011428-Transformers":{"title":"Transformers","content":"\n\u003c!-- \ntags: \n--\u003e\n\n\u003c!--internal\nparent:: [[]]\nchild:: [[]]\nrelated:: [[]]\n--\u003e\n\n\u003c!--external\n- [ ] [Assembly AI](https://www.youtube.com/@AssemblyAI)'s [Transformers for Beginners](https://www.youtube.com/watch?v=_UVfwBqcnbM)\n--\u003e\n\n# Transformers\n\n## Overview\n\n- Transformers were introduced in [[notes/202308090446 Attention is All You Need|Attention is All You Need]]\n- The power of the transformer architecture lies in its ability to learn the relevance and context not just of each word next to its neighbor ![[notes/images/Screenshot 2023-08-07 at 5.12.02 AM.png]] but to every other word in the sentence ![[notes/images/Screenshot 2023-08-07 at 5.13.21 AM.png]]\n- It is able to set attention weights to those relationships so that the model learns the relevance of each word to each other word no matter where it is in the input ![[notes/images/Screenshot 2023-08-07 at 5.06.37 AM.png]]!\n- This diagram above is called an attention map and is useful to illustrate the attention weights between each word and every other word ![[notes/images/Screenshot 2023-08-07 at 5.09.34 AM.png]]\n- The word book is strongly connected with or paying attention to the word teacher and the word student. This is called self-attention\n- \\Transformers use [[notes/20230703013343 Attention|Attention]] mechanisms instead of recurrence [[notes/20230703012211 Recurrent Neural Networks (RNNs)|Recurrent Neural Networks (RNNs)]] to remember information, making them faster and **parallelizable**\n","lastmodified":"2023-08-25T15:19:54.874738913Z","tags":[""]},"/notes/20230703012211-Recurrent-Neural-Networks-RNNs":{"title":"Recurrent Neural Networks (RNNs)","content":"\n\u003c!-- \ntags: \n--\u003e\n\n\u003c!--internal\nparent:: [[]]\nchild:: [[]]\nrelated:: [[]]\n--\u003e\n\n\u003c!--external\n- []()\n--\u003e\n\n# Recurrent Neural Networks (RNNs)\n\n![[notes/images/Screenshot 2023-08-07 at 4.53.07 AM.png]]\n## Limitations\n\n- with just one previous word available to the model, prediction was not good\n- As we scale the RNN to be able to see more of the preceding words in the text, we have to scale the resources that the model uses a lot and prediction would fail ![[notes/images/Screenshot 2023-08-07 at 4.44.41 AM.png]]![[notes/images/Screenshot 2023-08-07 at 4.45.06 AM.png]]\n\n- To predict the next word, models need to see more than just the previous few words.\n- Models needs to have an understanding of the whole sentence or even the whole document.\n- The problem is that language is complex. \n- In languages, one word can have multiple meanings i.e. homonyms\n- when provided a sentence, RNNs cannot remember the start of a sentence\n- RNNs use recurrence as result **could not be parallelized**","lastmodified":"2023-08-25T15:19:54.874738913Z","tags":[""]},"/notes/20230703012514-LSTMs":{"title":"LSTMs","content":"\n\u003c!-- \ntags: \n--\u003e\n\n\u003c!--internal\nparent:: [[]]\nchild:: [[]]\nrelated:: [[]]\n--\u003e\n\n\u003c!--external\n- []()\n--\u003e\n\n# LSTMs\n\n- LSTMs unlike [[notes/20230703012211 Recurrent Neural Networks (RNNs)|RNNs]] can remember\n- But LSTMs take too long to train","lastmodified":"2023-08-25T15:19:54.874738913Z","tags":[""]},"/notes/20230703013343-Attention":{"title":"Attention","content":"\n\u003c!-- \ntags: \n--\u003e\n\n\u003c!--internal\nparent:: [[]]\nchild:: [[notes/20230703011428 Transformers|Transformer]]\nrelated:: [[]]\n--\u003e\n\n\u003c!--external\n- []()\n--\u003e\n\n# Attention\n\nAttention allows the model to focus on important parts of the input, such as words in a sentence or features in an image\n\n","lastmodified":"2023-08-25T15:19:54.874738913Z","tags":[""]},"/notes/20230703031649-Word-Embedding":{"title":"Word Embedding","content":"\n\u003c!-- \ntags: \n--\u003e\n\n\u003c!--internal\nparent:: [[]]\nchild:: [[]]\nrelated:: [[]]\n--\u003e\n\n\u003c!--external\n- []()\n--\u003e\n\n# Word Embedding\n\nWord Embeddings are **vector representations of words** that **capture semantic and syntactic relationships between words**\n  \nconsider the following words and respective word embeddings in a 3-dimensional space:\n  \n- \"cat\": `[0.2, 0.4, 0.1]`\n- \"dog\": `[0.6, -0.3, 0.5]`\n- \"car\": `[0.8, 0.2, -0.6]`\n- \"bike\": `[0.7, -0.1, -0.4]`\n\nIn this example, each word is represented by a vector of 3 values, which can be interpreted as the word's features or characteristics. \n\nThe values in the embeddings are learned through training algorithms such as Word2Vec or GloVe, which aim to capture the meaning and context of words based on their co-occurrence patterns in a large corpus of text\n\nword embeddings allow the model to capture relationships between words. \n\nFor instance, in this example, \"cat\" and \"dog\" have similar embeddings with positive values in the first dimension, indicating that these 2 words share some semantic similarity. \n\non the other hand, \"car\" and \"bike\" have negative values in the third dimension, suggesting a potential contrast in their meanings.\n\n\u003cmark style=\"background: #BBFABBA6;\"\u003eword embeddings allow models to represent words as continuous vectors\u003c/mark\u003e and l\u003cmark style=\"background: #BBFABBA6;\"\u003eeverage the geometric properties of these vectors\u003c/mark\u003e to perform tasks like \u003cmark style=\"background: #FF5582A6;\"\u003esemantic similarity\u003c/mark\u003e, \u003cmark style=\"background: #FF5582A6;\"\u003eword analogy\u003c/mark\u003e, or \u003cmark style=\"background: #FF5582A6;\"\u003etext classification\u003c/mark\u003e","lastmodified":"2023-08-25T15:19:54.874738913Z","tags":["rn"]},"/notes/20230703031923-Positional-Encoding":{"title":"Positional Encoding","content":"\n\u003c!-- \ntags: \n--\u003e\n\n\u003c!--internal\nparent:: [[]]\nchild:: [[]]\nrelated:: [[]]\n--\u003e\n\n\u003c!--external\n- []()\n--\u003e\n\n# Positional Encoding\n\nPositional Encoding is used to provide information about the position of words within a sentence. \n\nConsider the sentence: \"The cat sat on the mat.\"\n\nwe first assign a unique position value to each word based on its position in the sentence\n\nwe assume each word is represented by a vector of length N (N=4)\n\nThe positional encoding can be represented as follows\n\n- \"The\" at position 1: `[0.8415, 0.5403, 0.0000, 0.0000]`\n- \"cat\" at position 2: `[0.9093, -0.4161, 0.0000, 0.0000]`\n- \"sat\" at position 3: `[-0.7568, -0.6536, 0.0000, 0.0000]`\n- \"on\" at position 4: `[-0.9589, 0.2837, 0.0000, 0.0000]`\n- \"the\" at position 5: `[0.1411, -0.9900, 0.0000, 0.0000]`\n- \"mat\" at position 6: `[-0.2794, 0.9602, 0.0000, 0.0000]`\n\nIn this example, each positional encoding vector contains four elements, representing different dimensions or features. \n\nThe values are determined using mathematical functions such as sine and cosine functions, to create distinct encoding patterns for each position\n\nThese positional encodings are then combined with word embeddings to provide the transformer model with both word-specific and position-specific information\n\nIt enables the transformer to understand the sequential structure of the input sentence.\n","lastmodified":"2023-08-25T15:19:54.874738913Z","tags":["rn"]},"/notes/20230703034916-Normalization":{"title":"Normalization","content":"\n\u003c!-- \ntags: \n--\u003e\n\n\u003c!--internal\nparent:: [[]]\nchild:: [[]]\nrelated:: [[]]\n--\u003e\n\n\u003c!--external\n- []()\n--\u003e\n\n# Normalization\n\nNormalization, in the context of machine learning, refers to the process of standardizing or scaling data to a common range or distribution. \n\nIt is often applied to features or variables in a dataset to ensure that the data have similar scales or magnitudes\n\nNormalization helps to avoid biases or undue influences that might arise from features with different scales. \n\nThe algorithm can compare and weigh the contributions of different features without a particular feature dominating the learning process based on its scale.\n\nThere are various methods of normalization, but most commonly used techniques are\n\n1.  Min-Max Scaling (Normalization): This method scales the values of a feature to a fixed range, typically between 0 and 1. It is achieved by subtracting the minimum value of the feature and dividing it by the difference between the maximum and minimum values. The formula for min-max scaling is: normalized\\_value = (value - min\\_value) / (max\\_value - min\\_value)\n    \n2.  Z-Score Standardization: This method transforms the values of a feature to have a mean of 0 and a standard deviation of 1. It is achieved by subtracting the mean of the feature and dividing it by the standard deviation. The formula for z-score standardization is: standardized\\_value = (value - mean) / standard\\_deviation\n\n**Normalization should be applied to numerical features, not categorical features**\n\nIt is important to note that **normalization is not always necessary or beneficial** for every machine learning algorithm. \n\nsome algorithms like **Decision Trees or Random Forests, are not affected by feature scales** unlike others, like Logistic Regression or Neural Networks\n","lastmodified":"2023-08-25T15:19:54.874738913Z","tags":["rn"]},"/notes/20230703043154-Vector-Database":{"title":"Vector Database","content":"\n\u003c!-- \ntags: \n--\u003e\n\n\u003c!--internal\nparent:: [[]]\nchild:: [[]]\nrelated:: [[]]\n--\u003e\n\n\u003c!--external\n- [ ] [A Gentle Introduction to Vector Databases](https://frankzliu.com/blog/a-gentle-introduction-to-vector-databases)\n- [ ] [Vector Databases Demystified: Part 1 - An Introduction to the World of High-Dimensional Data Storage](https://www.linkedin.com/pulse/vector-databases-demystified-part-1-introduction-world-adie-kaye)\n- [ ] [Vector Databases Demystified: Part 2 - Building Your Own (Very) Simple Vector Database in Python](https://www.linkedin.com/pulse/vector-databases-demystified-part-2-building-your-own-adie-kaye)\n- [ ] [Vector Databases Demystified: Part 3 - Build a colour matching app with Pinecone](https://www.linkedin.com/pulse/vector-databases-demystified-part-3-build-colour-matching-adie-kaye?trk=public_profile_article_view)\n- [ ] [Vector Databases Demystified: Part 4 - Using Sentence Transformers with Pinecone](https://www.linkedin.com/pulse/vector-databases-demystified-part-4-using-sentence-pinecone-kaye?trk=public_profile_article_view)\n- [ ] [Fine Tuning Your Own Sentence Transformers with Python](https://www.linkedin.com/pulse/fine-tuning-your-own-sentence-transformers-python-adie-kaye?trk=public_profile_article_view)\n- [ ] [What are your thoughts/experiences with building vector databases for AI/MLOps?](https://www.reddit.com/r/dataengineering/comments/m3hndj/what_are_your_thoughtsexperiences_with_building/)\n- [ ] [Benchmarking Nearest Neighbors](https://github.com/erikbern/ann-benchmarks)\n--\u003e\n\n# Vector Database\n\nThese databases contain arrays of numbers clustered together based on similarities, which can be queried with ultra-low latencies\n\nIn other words, vector databases index vectors for fast search and retrieval by comparing values and finding those that are most similar to one another.\n\nvector databases can used to store [[notes/20230703031649 Word Embedding|Embedding]]s and perform queries\n\nwe can use vector databases to \u003cmark style=\"background: #BBFABBA6;\"\u003eextend large language models with long-term memory\u003c/mark\u003e\n\n- start with general-purpose model, like GPT-4, [LLaMA](https://github.com/facebookresearch/llama), or [LaMDA](https://blog.google/technology/ai/lamda/) but provide custom data in a vector database. \n- When a user gives a prompt, we can run queries for relevant documents in the database to update the context, which will customize the final response. \n\nvector databases integrate with tools like [[notes/20230703061520 LangChain|LangChain]] that combine multiple LLMs together\n\n## List of Vector Databases\n\n### open-source\n\n  - [[notes/20230703070807 PgVector|PgVector]]\n  - [chroma](https://github.com/chroma-core/chroma)\n  - [weaviate](https://github.com/weaviate/weaviate)\n  - [milvus](https://github.com/milvus-io/milvus)\n  - [qrant](https://github.com/qdrant/qdrant)\n  - [faiss](https://github.com/facebookresearch/faiss)\n  - [llama_index](https://github.com/jerryjliu/llama_index)\n\n### closed-source\n\n  - [pinecone](https://www.pinecone.io)\n","lastmodified":"2023-08-25T15:19:54.874738913Z","tags":[""]},"/notes/20230703060600-Build-Your-Own":{"title":"Build Your Own","content":"\n\u003c!-- \ntags: \n--\u003e\n\n\u003c!--internal\nparent:: [[]]\nchild:: [[]]\nrelated:: [[]]\n--\u003e\n\n\u003c!--external\n- [ ] []()\n--\u003e\n\n# Build Your Own\n\n- [ ] [[notes/20230628031147 Generative Pretrained Transformer (GPT)|GPT]]\n  - [ ] [nanoGPT](https://github.com/karpathy/nanoGPT) \n  - [ ] [miniGPT](https://github.com/karpathy/minGPT)\n- [ ] [[notes/20230703043154 Vector Database|Vector Database]]\n  - [ ] [Vector Databases Demystified: Part 2 - Building Your Own (Very) Simple Vector Database in Python](https://www.linkedin.com/pulse/vector-databases-demystified-part-2-building-your-own-adie-kaye)","lastmodified":"2023-08-25T15:19:54.874738913Z","tags":[""]},"/notes/20230703061520-LangChain":{"title":"LangChain","content":"\n\u003c!-- \ntags: \n--\u003e\n\n\u003c!--internal\nparent:: [[]]\nchild:: [[]]\nrelated:: [[]]\n--\u003e\n\n\u003c!--external\n- [ ] []()\n--\u003e\n\n#  LangChain\n\n[LangChain](https://github.com/hwchase17/langchain) is a framework for developing applications powered by language models\n\n- It can call a language model via an API\n- It can also connect [[notes/20230628030810 Large Language Models (LLMs)|LLMs]], like GPT-4, LLaMDA, and LLaMA, to other sources of data such as Google Drive, Wikipedia and allow them to interact with them \n- It can chain commands together so the AI model can know what it needs to do to produce the results or perform the actions needed\n\n## Features\n\n![[notes/20230703063700 Agents|Agents]]\n\n![[notes/20230703063746 Memory|Memory]]","lastmodified":"2023-08-25T15:19:54.874738913Z","tags":[""]},"/notes/20230703063700-Agents":{"title":"Agents","content":"\n\u003c!-- \ntags: \n--\u003e\n\n\u003c!--internal\nparent:: [[notes/20230703061520 LangChain|LangChain]]\nchild:: [[]]\nrelated:: [[]]\n--\u003e\n\n\u003c!--external\n- [ ] []()\n--\u003e\n\n# Agents\n\nAgents are a method of using a language model as a reasoning engine to determine how to interact with the outside world based on the user's input.\n\nAgents have access to a suite of tools and, depending on the input, an agent can decide which tools to call.","lastmodified":"2023-08-25T15:19:54.874738913Z","tags":[""]},"/notes/20230703063746-Memory":{"title":"Memory","content":"\n\u003c!-- \ntags: \n--\u003e\n\n\u003c!--internal\nparent:: [[notes/20230703061520 LangChain|LangChain]]\nchild:: [[]]\nrelated:: [[]]\n--\u003e\n\n\u003c!--external\n- [ ] []()\n--\u003e\n\n# Memory\n\nLLMs process each query independently of other interactions. \n\nLangChain provides memory components to manage and manipulate previous chat messages and incorporate them into chains. \n\nLangChain's memory components can be used to retrieve data from memory or store data in memory. \n\nThis is important for chatbots, for example, which need to remember previous conversations.","lastmodified":"2023-08-25T15:19:54.874738913Z","tags":[""]},"/notes/20230703063839-Alternatives-to-LangChain":{"title":"Alternatives to LangChain","content":"\n\u003c!-- \ntags: \n--\u003e\n\n\u003c!--internal\nparent:: [[]]\nchild:: [[]]\nrelated:: [[]]\n--\u003e\n\n\u003c!--external\n- [ ] []()\n--\u003e\n\n# Alternatives to LangChain\n\n- [AutoGPT](https://autogpt.net)\n  - Auto-GPT has tendencies where it gets stuck in infinite logic loops and rabbit holes\n- [AgentGPT](https://agentgpt.reworkd.ai/)\n- [BabyAGI](https://github.com/yoheinakajima/babyagi)\n- [LangDock](https://www.langdock.com)\n- [GradientJ](https://gradientj.com)\n- [FlowiseAI](https://flowiseai.com)\n- [TensorFlow](https://www.tensorflow.org/)","lastmodified":"2023-08-25T15:19:54.874738913Z","tags":[""]},"/notes/20230703070807-PgVector":{"title":"PgVector","content":"\n\u003c!-- \ntags: \n--\u003e\n\n\u003c!--internal\nparent:: [[]]\nchild:: [[]]\nrelated:: [[]]\n--\u003e\n\n\u003c!--external\n- [ ] []()\n--\u003e\n\n# PgVector\n\n- [PgVector](https://github.com/pgvector/pgvector) supports up to 16,000 dimensions for storage and 2,000 for indexing\n- PgVector supports exact KNN which is what most apps just starting their journey (\u003c10k docs) need\n- It supports IVF ANN for larger collections\n- postgres natively supports several additional composite index types, including BTREE GIST and GIN, for tabular, text and json data in addition to vector indexes.\n- SQL databases in general (along w/ OpenSearch) are significantly (orders of magnitude) ahead of Pinecone in terms of scalability which limits a pod size to ~5M vectors\n- The HNSW algorithmic advantage may be 2-10x as fast as IVF __in memory__, but it's \u003cmark style=\"background: #BBFABBA6;\"\u003e100x slower to go over the network to a different datacenter like Pinecone \u003c/mark\u003ethan using the database in the same VPC, so that \"advantage\" is situational at best\n- [[notes/20230703073417 PostgresML|PostgresML]] takes another network trip out of the equation\n- Beside the simple vector recall, It runs the embedding model itself on input text inside the same memory space","lastmodified":"2023-08-25T15:19:54.874738913Z","tags":[""]},"/notes/20230703073417-PostgresML":{"title":"PostgresML","content":"\n\u003c!-- \ntags: \n--\u003e\n\n\u003c!--internal\nparent:: [[]]\nchild:: [[]]\nrelated:: [[]]\n--\u003e\n\n\u003c!--external\n- [ ] [Generating LLM embeddings with open source models in PostgresML](https://postgresml.org/blog/generating-llm-embeddings-with-open-source-models-in-postgresml)\n--\u003e\n\n# [PostgresML](https://github.com/postgresml/postgresml)\n","lastmodified":"2023-08-25T15:19:54.874738913Z","tags":[""]},"/notes/20230703150959-Usecases":{"title":"Usecases","content":"\n\u003c!-- \ntags: \n--\u003e\n\n\u003c!--internal\nparent:: [[]]\nchild:: [[]]\nrelated:: [[]]\n--\u003e\n\n\u003c!--external\n- [ ] []()\n--\u003e\n\n# Usecases\n\n\n- Generative AI for concept arts\n  - tinder like interface for human review/feedback\n  - track line of sight to build heatmaps of attention (reminds me of the paper spandan worked on)","lastmodified":"2023-08-25T15:19:54.874738913Z","tags":[""]},"/notes/20230711082511-Campaign-Creation-to-Tactic-Authoring":{"title":"Campaign Creation to Tactic Authoring","content":"\n\u003c!-- \ntags: \n--\u003e\n\n\u003c!--internal\nparent:: [[]]\nchild:: [[]]\nrelated:: [[]]\n--\u003e\n\n\u003c!--external\n- [ ] []()\n--\u003e\n\n# Campaign Creation to Tactic Authoring\n\n![[notes/images/Screenshot 2023-07-11 at 8.27.57 AM.png]]\n\n## Inputs\n\n- Custom Built Compliant Prompt\n\n## Planes\n\n- campaign strategy creation\n  - ChatGPT\n- content copy creation\n  - ChatGPT\n- Realistic \u0026 Brand Compliant Image Creation\n  - MidJourney\n\n## Output\n\n- HTML Code Generation\n- Responsive\n- Brand Compilant","lastmodified":"2023-08-25T15:19:54.874738913Z","tags":[""]},"/notes/20230711101408-Content-Repurposing-Email-to-Video":{"title":"Content Repurposing (Email to Video)","content":"\n\u003c!-- \ntags: \n--\u003e\n\n\u003c!--internal\nparent:: [[]]\nchild:: [[]]\nrelated:: [[]]\n--\u003e\n\n\u003c!--external\n- [ ] []()\n--\u003e\n\n# Content Repurposing (Email to Video)\n\n![[notes/images/Screenshot 2023-07-11 at 1.46.22 PM.png]]\n\n- provide prompt as text\n- ask ChatGPT to write me a script/scene\n- use [[notes/20230711171651 MidJourneyAI|MidJourneyAI]] to create images\n- use [[notes/20230711172239 Eleven Labs|Eleven Labs]] speech synthesis to generate audio\n- animate each character using [[notes/20230711172730 D-ID AI|D-ID AI]]\n- ffmpeg or video editor (like ) to join clip together\n\n## Tools/Services\n\n- Langchain/OpenAI (create text)\n- Happyaccidents/playgrounds/leonardoai/python script (Text to image)\n- Synthesia (AI Video generator) (D-ID, Heygen, Invideo, RunawayML: Didn't create/play the video)","lastmodified":"2023-08-25T15:19:54.874738913Z","tags":[""]},"/notes/20230711171651-MidJourneyAI":{"title":"MidJourneyAI","content":"\n\u003c!-- \ntags: \n--\u003e\n\n\u003c!--internal\nparent:: [[]]\nchild:: [[]]\nrelated:: [[]]\n--\u003e\n\n\u003c!--external\n- [ ] []()\n--\u003e\n\n# MidJourney AI\n\nannual subscription cost for basic plan cost $96\n\n## Tips\n\n- how to change the aspect ratio of images\n  - include “—ar 3:2” in the prompt\n  - include “full body” if you don’t want super close-ups\n- add term “cinematic,” to prompts\n- add specific dates for reference.\n\n## Prompts\n\n\u003e “Frodo Baggins, portrait, full body, cinematic, film still, in the style of a Wes Anderson live-action movie circa 2008 —ar 3:2.”\n\n\u003e Owen Wilson as Legolas the elf, portrait, full body, cinematic, holding a bow and arrow, symmetrical, facing forward, film still, exterior shot, daytime, in the style of a Wes Anderson live-action movie circa 2008 —ar 3:2.”\n","lastmodified":"2023-08-25T15:19:54.874738913Z","tags":[""]},"/notes/20230711172239-Eleven-Labs":{"title":"Eleven Labs","content":"\n\u003c!-- \ntags: \n--\u003e\n\n\u003c!--internal\nparent:: [[]]\nchild:: [[]]\nrelated:: [[]]\n--\u003e\n\n\u003c!--external\n- [ ] []()\n--\u003e\n\n# [Elevenlabs](https://beta.elevenlabs.io/) \n\n$5 per month\n\n- requires clips where there was zero background noise and voice is clear\n- how did automatic detect background noise","lastmodified":"2023-08-25T15:19:54.874738913Z","tags":[""]},"/notes/20230711172730-D-ID-AI":{"title":"D-ID AI","content":"\n\u003c!-- \ntags: \n--\u003e\n\n\u003c!--internal\nparent:: [[]]\nchild:: [[]]\nrelated:: [[]]\n--\u003e\n\n\u003c!--external\n- [ ] []()\n--\u003e\n\n# D-ID AI \n\n$4.99 per month\n\nwhere you can either type out a script for each character to say or upload an existing sound bite. \n\nI did the latter for Frodo and Gandalf, and for the other characters who didn’t have speaking roles but still needed to look, y’know, alive, I inserted a series of “pauses” into their speech box. \n\ncharacters blink and move heads around a bit","lastmodified":"2023-08-25T15:19:54.874738913Z","tags":[""]},"/notes/202308051230-Generative-AI-with-Large-Language-Models":{"title":"202308051230 Generative AI with Large Language Models","content":"\n\u003c!--\ntags: \n--\u003e\n\n\u003c!--internal\nparent:: [[]]\nchild:: [[]]\nrelated:: [[]]\n--\u003e\n\n\u003c!--external\n- [ ] []()\n--\u003e\n\n# Generative AI with Large Language Models\n\n## Objectives\n\n- Discuss model pre-training and the value of continued pre-training vs fine-tuning\n- Define the terms [[notes/20230628030901 Generative AI|Generative AI]], [[notes/20230628030810 Large Language Models (LLMs)|Large Language Models]], prompt, and describe the  [[notes/20230703011428 Transformers|Transformer]] architecture that powers LLMs\n- Describe the steps in a typical LLM-based, generative AI model lifecycle and discuss the constraining factors that drive decisions at each step of model lifecycle\n- Discuss computational challenges during model pre-training and determine how to efficiently reduce memory footprint\n- Define the term scaling law and describe the laws that have been discovered for LLMs related to training dataset size, compute budget, inference requirements, and other factors.\n\n## Prerequisites\n\n- Python\n- Tensorflow\n\n## Timeline\n### Week 1\n\n- Examine [[notes/20230703011428 Transformers|Transformer]] Architecture, Explore how these models are trained and compute resources required\n- [[notes/202308061627 In-context Learning|In-context Learning]] to guide the model to output at inference time using prompt engineering\n\n### Week 2\n\n- Explore options adapt pre-trained models to specific tasks and datasets using a process called [[notes/202308061628 Instruction Fine Tuning|Instruction Fine Tuning]]\n- How to fine tune smaller models (~1-30 B parameters) can perform well on specific focussed tasks\n\n### Week 3\n\n- How to augment existing LLMs with connection to external data sources\n- How to align outputs of language models with human values in order to increase helpfulness and reduce potential harm i.e. [[notes/202308051314 Reinforced Learning from Human Feedback (RLHF)|RLHF]]\n","lastmodified":"2023-08-25T15:19:54.874738913Z","tags":[""]},"/notes/202308051314-Reinforced-Learning-from-Human-Feedback-RLHF":{"title":"202308051314 Reinforced Learning from Human Feedback (RLHF)","content":"\n\u003c!--\ntags: \n--\u003e\n\n\u003c!--internal\nparent:: [[]]\nchild:: [[]]\nrelated:: [[]]\n--\u003e\n\n\u003c!--external\n- [ ] []()\n--\u003e\n\n#  Reinforced Learning from Human Feedback (RLHF)","lastmodified":"2023-08-25T15:19:54.874738913Z","tags":[""]},"/notes/202308061603-Research-Papers":{"title":"202308061603 Research Papers","content":"\n\u003c!--\ntags: \n--\u003e\n\n\u003c!--internal\nparent:: [[]]\nchild:: [[]]\nrelated:: [[]]\n--\u003e\n\n\u003c!--external\n- [ ] []()\n--\u003e\n\n# Research Papers\n\n- [ ] [[notes/202308090446 Attention is All You Need|Attention is All You Need]]\n\t- [ ] It allowed attention to work in parallel manner at a massive scale","lastmodified":"2023-08-25T15:19:54.874738913Z","tags":[""]},"/notes/202308061627-In-context-Learning":{"title":"202308061627 In-context Learning","content":"\n\u003c!--\ntags: \n--\u003e\n\n\u003c!--internal\nparent:: [[]]\nchild:: [[]]\nrelated:: [[]]\n--\u003e\n\n\u003c!--external\n- [ ] []()\n--\u003e\n\n# In-context Learning\n","lastmodified":"2023-08-25T15:19:54.874738913Z","tags":[""]},"/notes/202308061628-Instruction-Fine-Tuning":{"title":"202308061628 Instruction Fine Tuning","content":"\n\u003c!--\ntags: \n--\u003e\n\n\u003c!--internal\nparent:: [[]]\nchild:: [[]]\nrelated:: [[]]\n--\u003e\n\n\u003c!--external\n- [ ] []()\n--\u003e\n#  Instruction Fine Tuning\n\n[[notes/202308051314 Reinforced Learning from Human Feedback (RLHF)|RLHF]]","lastmodified":"2023-08-25T15:19:54.874738913Z","tags":[""]},"/notes/202308061649-Generative-AI-project-Lifecycle":{"title":"202308061649 Generative AI project Lifecycle","content":"\n\u003c!--\ntags: \n--\u003e\n\n\u003c!--internal\nparent:: [[]]\nchild:: [[]]\nrelated:: [[]]\n--\u003e\n\n\u003c!--external\n- [ ] []()\n--\u003e\n\n# Generative AI project Life Cycle\n\nBuild a good intuition about the important decisions to make, the potential difficulties, and the infrastructure needed to develop and serve the application\n\nThe [[notes/20230628030901 Generative AI|Generative AI]] project Lifecycle is a guide through the individual stages and decisions we have to make when developing Generative AI applications\n\n![[notes/images/Screenshot 2023-08-10 at 6.13.12 AM.png]]\n\nThis framework maps out the tasks required to take the project from conception to launch. \n\n![[notes/20230810063625 Scope Stage|Scope Stage]]\n\n![[notes/20230810063546 Select Stage|Select Stage]]\n\n![[notes/20230810063457 Adapt and Align stage|Adapt and Align stage]]\n\n![[notes/20230810063410 Application Integration Stage|Application Integration]]\n\n\n","lastmodified":"2023-08-25T15:19:54.874738913Z","tags":[""]},"/notes/20230806171025-Foundational-Models":{"title":"20230806171025 Foundational Models","content":"\n\u003c!-- \ntags: \n--\u003e\n\n\u003c!--internal\nparent:: [[20230628030810 Large Language Models (LLMs)]]\nchild:: [[]]\nrelated:: [[]]\n--\u003e\n\n\u003c!--external\n- [ ] []()\n--\u003e\n\n# Foundational Models\n\nThe choice is whether to use Foundational [[notes/20230628030810 Large Language Models (LLMs)|LLMs]] as it is or perform instruction fine tuning to tailor it to a particular task\n\n- BERT (100M)\n- BLOOM (176B)\n- GPT\n- LLaMa\n- FLAN-T5\n- PaLM\n\n","lastmodified":"2023-08-25T15:19:54.874738913Z","tags":["rn"]},"/notes/20230807051721-Transformer-Architecture":{"title":"20230807051721 Transformer Architecture","content":"\n\u003c!-- \ntags: \n--\u003e\n\n\u003c!--internal\nparent:: [[20230703011428 Transformers]]\nchild:: [[]]\nrelated:: [[]]\n--\u003e\n\n\u003c!--external\n- [ ] []()\n--\u003e\n# Transformer Architecture\n\n![[notes/images/Screenshot 2023-07-03 at 1.28.32 AM.png|250]]\n\n- The [[notes/20230703011428 Transformers|Transformer]] architecture is split into two distinct parts, the [[notes/202308090432 Encoder|Encoder]] and the [[notes/202308090433 Decoder|Decoder]]\n ![[notes/images/Screenshot 2023-08-07 at 5.24.45 AM.png|250]]\n- These components work in conjunction with each other and share a number of similarities\n- The architecture of a [[notes/20230703011428 Transformers|Transformer]] includes \u003cmark style=\"background: #FF5582A6;\"\u003esix\u003c/mark\u003e encoders and decoders\n- each encoder has \n\t- 1 multi-head self-attention and \n\t- 1 feed-forward neural network\n- each decoder has \n\t- 1 multi-head self-attention, \n\t- 1 masked multi-head attention and \n\t- 1 feed-forward neural network\n- In between each sub-layer, there are Add and Normalized layer to [[notes/20230703034916 Normalization|Normalize]] the output out of the sub-layer\n\t- The normalization technique used here is called \u003cmark style=\"background: #FF5582A6;\"\u003eLayer Normalization\u003c/mark\u003e\n- The words in the text are tokenized\n\t- we can choose from multiple tokenization methods \n\t- For example, token IDs can match two complete words, or represent parts of words\n\t- **It important is that once we've selected a tokenizer to train the model, we must use the same tokenizer to generate text.**\n- The model processes each of the input tokens in parallel\n- The tokens are [[notes/20230703031649 Word Embedding|Embedded]] \n\t- Each token is represented as a vector and occupies a unique location within that high-dimensional space trainable vector space\n\t- Each token ID in the vocabulary is matched to a multi-dimensional vector ![[notes/images/Screenshot 2023-08-07 at 6.00.01 AM.png|500]]\n\t- **The intuition is that these vectors will learn to encode the meaning and context of individual tokens in the input sequence**\n- The tokens also undergo [[notes/20230703031923 Positional Encoding|Positional Encoded]] ![[notes/images/Screenshot 2023-08-07 at 6.40.06 AM.png]]\n- **Parallelization** is performed with feeding all words (tokenized) of the sentence through the \u003cmark style=\"background: #FF5582A6;\"\u003emulti-head self-attention layer\u003c/mark\u003e\n\t- self-attention allows the model to attend to different parts of the input sequence to better capture the contextual dependencies between the words. \n\t- The weights that are learned during training and stored in these layers reflect the importance of each word in that input sequence to all other words in the sequence. \n\t- But it does not happen just once, the transformer architecture has multi-headed self-attention\n\t- This means that multiple sets of self-attention weights or heads are learned in parallel independent of each other.\n\t- The intuition here is that each self-attention head will learn a different aspect of language for example, one head might see the relationship between the people entities in our sentence while another head might focus on the activity of the sentence, yet another head might focus whether the words rhyme. \n\t- It is important to note that we don't dictate ahead of time what aspects of language the attention heads will learn\n\t- The weights of each head are randomly initialized and given sufficient training data and time, each will learn different aspects of language. \n- After all of the attention weights have been applied to input data, the output is processed through a fully-connected feed-forward network.\n- The output of FFF network is a vector of logits proportional to the probability score for each and every token in the tokenizer dictionary\n- This vector is logit is passed to a final **softmax** layer, where these logits are normalized into a probability score for each word\n- The output of the softmax includes a probability for every single word in the vocabulary, so there's likely to be thousands of scores here. \n- one single token will have a score higher than the rest which is likeliest predicted token. \n- However, there are a number of methods that you can use to vary the final selection from this vector of probabilities.\n","lastmodified":"2023-08-25T15:19:54.874738913Z","tags":["rn"]},"/notes/202308090324-Generating-Text-with-Transformers":{"title":"202308090324 Generating Text with Transformers","content":"\n\u003c!--\ntags: \n--\u003e\n\n\u003c!--internal\nparent:: [[]]\nchild:: [[]]\nrelated:: [[]]\n--\u003e\n\n\u003c!--external\n- [ ] []()\n--\u003e\n# Generating Text with Transformers\n\nThe complete [[notes/20230807051721 Transformer Architecture|Transformer Architecture]] consists of an [[notes/202308090432 Encoder|Encoder]] and [[notes/202308090433 Decoder|Decoder]] components\n\nHow does overall prediction process works from end to end for a translation (or sequence-to-sequence) task\n\nTranslate the French phrase `J'aime l'apprentissage automatique` into English using [[notes/20230809041643 Encoder-Decoder Models|Encoder-Decoder Model]]\n## Procedure\n\n### Encoder\n\n- Tokenize the input words using this same tokenizer hat was used to train the network\n- These tokens are then added into the input on the encoder side of the network and passed through the embedding layer \n- The output of embedding layer are then fed into the multi-headed attention layers\n- The outputs of the multi-headed attention layers are fed through a feed-forward network to the output of the encoder\n- \u003cmark style=\"background: #BBFABBA6;\"\u003eThe data that leaves the encoder\u003c/mark\u003e is a **deep representation of the structure and meaning of the input sequence**\n\n\t![[notes/images/Screenshot 2023-08-09 at 4.23.07 AM.png|650]]\n\n### Decoder\n\nThe output of the encoder is \u003cmark style=\"background: #BBFABBA6;\"\u003einserted into the middle of the decoder\u003c/mark\u003e to influence the decoder's self-attention mechanisms\n\n- A start of sequence token is added to the input of the decoder\n- It triggers the decoder to predict the next token based on the contextual understanding that it's being provided from the encoder\n- The output of the decoder's self-attention layers gets passed through the decoder feed-forward network\n- The output of decoder feed forward networks is passed through a final softmax output layer to generate the first output token\n- This process **\u003cmark style=\"background: #BBFABBA6;\"\u003eruns in a loop\u003c/mark\u003e passing the output token back to the input to trigger the generation of the next token \u003cmark style=\"background: #BBFABBA6;\"\u003euntil the model predicts an end-of-sequence token\u003c/mark\u003e\n\n\t![[notes/images/Screenshot 2023-08-09 at 4.28.40 AM.png|650]]\n\nThe final sequence of tokens can be de-tokenized into words to get the output\n\n\t![[notes/images/Screenshot 2023-08-09 at 4.29.31 AM.png|650]]\n## Considerations\n\n- There are \u003cmark style=\"background: #BBFABBA6;\"\u003emultiple methods to use the output from the softmax layer\u003c/mark\u003e to predict the next token which can \u003cmark style=\"background: #BBFABBA6;\"\u003einfluence how creative the generated text is\u003c/mark\u003e\n## Variations\n\n- [[notes/20230809041717 Encoder-only Models|Encoder-only Models]] can do sequence-to-sequence task but sequences will be of same length\n- [[notes/20230809040949 Decoder-only Models|Decoder-only Models]] generalize to most tasks with scale\n","lastmodified":"2023-08-25T15:19:54.874738913Z","tags":[""]},"/notes/20230809040949-Decoder-only-Models":{"title":"20230809040949 Decoder-only Models","content":"\n\u003c!-- \ntags: \n--\u003e\n\n\u003c!--internal\nparent:: [[202308090324 Generating Text with Transformers]]\nchild:: [[]]\nrelated:: [[]]\n--\u003e\n\n\u003c!--external\n- [ ] []()\n--\u003e\n\n# Decoder-only Models\n\n![[notes/images/Screenshot 2023-08-09 at 4.44.22 AM.png|250]]\n\nHow do [[notes/202308090433 Decoder|Decoder]]-only models (like GPT, BLOOM, Jurassic, LLaMA) generalize to most tasks with scale #TODO \n","lastmodified":"2023-08-25T15:19:54.874738913Z","tags":["rn"]},"/notes/20230809041643-Encoder-Decoder-Models":{"title":"20230809041643 Encoder-Decoder Models","content":"\n\u003c!-- \ntags: \n--\u003e\n\n\u003c!--internal\nparent:: [[202308090324 Generating Text with Transformers]]\nchild:: [[]]\nrelated:: [[]]\n--\u003e\n\n\u003c!--external\n- [ ] []()\n--\u003e\n\n# Encoder-Decoder Models\n\n- Encoder-decoder models (like BART) perform well on sequence-to-sequence tasks such as translation, where the input sequence and the output sequence can be different lengths. \n- You can also scale and train this type of model to perform general text generation tasks. \n- Examples of encoder-decoder models include BART as opposed to BERT and T5, the model that you'll use in the labs in this course. ","lastmodified":"2023-08-25T15:19:54.874738913Z","tags":["rn"]},"/notes/20230809041717-Encoder-only-Models":{"title":"20230809041717 Encoder-only Models","content":"\n\u003c!-- \ntags: \n--\u003e\n\n\u003c!--internal\nparent:: [[202308090324 Generating Text with Transformers]]\nchild:: [[]]\nrelated:: [[]]\n--\u003e\n\n\u003c!--external\n- [ ] []()\n--\u003e\n\n# Encoder-only ModelsX\n\n- [[notes/202308090432 Encoder|Encoder]]-only models (like BERT) also work as sequence-to-sequence models where the input sequence and the output sequence are of the same length\n- With additional layers to the architecture, It is possible to train encoder-only models to perform classification tasks such as sentiment \n\n![[notes/images/Screenshot 2023-08-09 at 4.43.01 AM.png|250]]","lastmodified":"2023-08-25T15:19:54.874738913Z","tags":["rn"]},"/notes/202308090432-Encoder":{"title":"202308090432 Encoder","content":"\n\u003c!--\ntags: \n--\u003e\n\n\u003c!--internal\nparent:: [[notes/20230703011428 Transformers|Transformer]]\nchild:: [[]]\nrelated:: [[]]\n--\u003e\n\n\u003c!--external\n- [ ] []()\n--\u003e\n\n# Encoder\n\nThe encoder encodes input sequences into a deep representation of the structure and meaning of the input","lastmodified":"2023-08-25T15:19:54.874738913Z","tags":[""]},"/notes/202308090433-Decoder":{"title":"202308090433 Decoder","content":"\n\u003c!--\ntags: \n--\u003e\n\n\u003c!--internal\nparent:: [[notes/20230703011428 Transformers|Transformer]]\nchild:: [[]]\nrelated:: [[]]\n--\u003e\n\n\u003c!--external\n- [ ] []()\n--\u003e\n\n# Decoder\n\nThe decoder, working from input token triggers, uses the encoder's contextual understanding to generate new tokens. It does this in a loop until some stop condition has been reached. ","lastmodified":"2023-08-25T15:19:54.874738913Z","tags":[""]},"/notes/202308090446-Attention-is-All-You-Need":{"title":"202308090446 Attention is All You Need","content":"\n\u003c!--\ntags: \n--\u003e\n\n\u003c!--internal\nparent:: [[]]\nchild:: [[]]\nrelated:: [[notes/202308061603 Research Papers|Research Paper]]\n--\u003e\n\n\u003c!--external\n- [ ] [Transformer — Attention Is All You Need Easily Explained With Illustrations](https://luv-bansal.medium.com/transformer-attention-is-all-you-need-easily-explained-with-illustrations-d38fdb06d7db#:~:text=A%20Transformer%20is%20a%20type,top%20of%20the%20transformer%20model.)\n- [ ] [Attention is all you need: understanding with example](https://medium.com/data-science-in-your-pocket/attention-is-all-you-need-understanding-with-example-c8d074c37767)\n- [ ] [Attention is all you need: Discovering the Transformer paper](https://towardsdatascience.com/attention-is-all-you-need-discovering-the-transformer-paper-73e5ff5e0634)\n--\u003e\n\n# [Attention Is All You Need](https://arxiv.org/abs/1706.03762)\n\n![[notes/images/Pasted image 20230809044742.png]]\n\nThe Transformer architecture consists of an encoder and a decoder, each of which is composed of several layers\n\nEach layer consists of two sub-layers\n- A multi-head self-attention mechanism and \n- A feed-forward neural network. \n\nThe multi-head self-attention mechanism allows the model to attend to different parts of the input sequence\n\nThe feed-forward network applies a point-wise fully connected layer to each position separately and identically\n\nThe Transformer model a uses residual connections and layer normalization to facilitate training and prevent overfitting. \n\nIn addition, the authors introduce a positional encoding scheme that encodes the position of each token in the input sequence, enabling the model to capture the order of the sequence\n","lastmodified":"2023-08-25T15:19:54.874738913Z","tags":["rn"]},"/notes/202308090519-Prompt-Engineering":{"title":"202308090519 Prompt Engineering","content":"\n\u003c!--\ntags: \n--\u003e\n\n\u003c!--internal\nparent:: [[]]\nchild:: [[]]\nrelated:: [[]]\n--\u003e\n\n\u003c!--external\n- [ ] []()\n--\u003e\n\n# Prompt Engineering\n\n**Prompts** are the text that we feed into the model. The **act of generating text** is known as **Inference** and the **output text** is known as the **completion**\n\nThe model doesn't produce the desired outcome the first attempt **quite often** so we revise the language in the prompt several times \n\nThis practise of developing and improving the prompt is known as **Prompt Engineering**","lastmodified":"2023-08-25T15:19:54.874738913Z","tags":[""]},"/notes/202308090550-Context-Window":{"title":"202308090550 Context Window","content":"\n\u003c!--\ntags: \n--\u003e\n\n\u003c!--internal\nparent:: [[]]\nchild:: [[]]\nrelated:: [[]]\n--\u003e\n\n\u003c!--external\n- [ ] []()\n--\u003e\n\n# Context Window\n\nThe full amount of text or the **memory available** to use for the prompt is called the **Context Window**.","lastmodified":"2023-08-25T15:19:54.874738913Z","tags":[""]},"/notes/20230809055132-In-Context-Learning-ICL":{"title":"20230809055132 In-Context Learning (ICL)","content":"\n\u003c!-- \ntags: \n--\u003e\n\n\u003c!--internal\nparent:: [[202308090519 Prompt Engineering]]\nchild:: [[]]\nrelated:: [[]]\n--\u003e\n\n\u003c!--external\n- [ ] []()\n--\u003e\n\n# In-Context Learning (ICL)\n\n**In-Context Learning** is the practise of providing examples inside the [[notes/202308090550 Context Window|Context Window]] of a Prompt in order to get the model to produce better outcomes. It is useful in [[notes/202308090519 Prompt Engineering|Prompt Engineering]]\n\n- [[notes/20230809055419 Zero-Shot Inference|Zero-Shot Inference]]\n- [[notes/20230809055340 One-Shot Inference|One-Shot Inference]]\n- [[notes/20230809055214 Few-Shot Inference|Few-Shot Inference]]\n\nHowever, we have a **limit on the amount of examples** we can provide for in-context learning that due to the **size of the context window**\n\nif the model isn't performing well even after including five or six examples, consider [[notes/202308061628 Instruction Fine Tuning|Fine Tuning]] the model instead\n\n\n","lastmodified":"2023-08-25T15:19:54.874738913Z","tags":["rn"]},"/notes/20230809055214-Few-Shot-Inference":{"title":"20230809055214 Few-Shot Inference","content":"\n\u003c!-- \ntags: \n--\u003e\n\n\u003c!--internal\nparent:: [[20230809055132 In-Context Learning (ICL)]]\nchild:: [[]]\nrelated:: [[]]\n--\u003e\n\n\u003c!--external\n- [ ] []()\n--\u003e\n\n# Few-Shot Inference\n\nThe inclusion of a multiple examples is known as few-shot inference. A mix of examples with different output classes is preferred\n\n![[notes/images/Screenshot 2023-08-09 at 5.44.10 AM.png]]\n","lastmodified":"2023-08-25T15:19:54.874738913Z","tags":["rn"]},"/notes/20230809055340-One-Shot-Inference":{"title":"20230809055340 One-Shot Inference","content":"\n\u003c!-- \ntags: \n--\u003e\n\n\u003c!--internal\nparent:: [[20230809055132 In-Context Learning (ICL)]]\nchild:: [[]]\nrelated:: [[]]\n--\u003e\n\n\u003c!--external\n- [ ] []()\n--\u003e\n\n# One-Shot Inference\n\n![[notes/images/Screenshot 2023-08-09 at 5.39.09 AM.png]]\n\nThe inclusion of a single example is known as one-shot inference","lastmodified":"2023-08-25T15:19:54.874738913Z","tags":["rn"]},"/notes/20230809055419-Zero-Shot-Inference":{"title":"20230809055419 Zero-Shot Inference","content":"\n\u003c!-- \ntags: \n--\u003e\n\n\u003c!--internal\nparent:: [[20230809055132 In-Context Learning (ICL)]]\nchild:: [[]]\nrelated:: [[]]\n--\u003e\n\n\u003c!--external\n- [ ] []()\n--\u003e\n\n# Zero-Shot Inference\n\n![[notes/images/Screenshot 2023-08-09 at 5.34.39 AM.png]]\n\nThe prompt consists of the instruction, \"classify this review,\" followed by some context in this case is the review text itself, and an instruction to produce the sentiment at the end\n\nThis method of input data within the prompt, is called **Zero-Shot Inference**\n\nThe largest LLMs are great at it but smaller models can struggle with it","lastmodified":"2023-08-25T15:19:54.874738913Z","tags":["rn"]},"/notes/202308100523-Generative-Configuration":{"title":"202308100523 Generative Configuration","content":"\n\u003c!--\ntags: \n--\u003e\n\n\u003c!--internal\nparent:: [[]]\nchild:: [[]]\nrelated:: [[]]\n--\u003e\n\n\u003c!--external\n- [ ] []()\n--\u003e\n\n# Generative Configuration\n\nEach [[notes/20230628030901 Generative AI|Generative]] model exposes a set of configuration parameters that can influence the model's output during inference. \n\nThese are different than the training parameters which are learned during training time. \n\nInstead, these configuration parameters are invoked at inference time and give us control over things like the maximum number of tokens in the completion, and how creative the output is\n\n- [[notes/202308100605 Max New Token|Max New Token]]\n- [[notes/20230810055942 Top-K sampling|Top-K value]]\n- [[notes/20230810055842 Top-P sampling|Top-P value]]\n- [[notes/20230810055747 Temperature|Temperature]]","lastmodified":"2023-08-25T15:19:54.874738913Z","tags":[""]},"/notes/20230810055747-Temperature":{"title":"20230810055747 Temperature","content":"\n\u003c!-- \ntags: \n--\u003e\n\n\u003c!--internal\nparent:: [[202308100523 Generative Configuration]]\nchild:: [[]]\nrelated:: [[]]\n--\u003e\n\n\u003c!--external\n- [ ] []()\n--\u003e\n\n# Temperature\n\nThe temperature parameter influences the shape of the probability distribution that the model calculates for the next token\n\nThe higher the temperature, the higher the randomness and the lower the temperature, the lower the randomness.\n\nThe temperature value is a scaling factor that's applied within the final softmax layer of the model that impacts the shape of the probability distribution of the next token. \n\nIn contrast to the top-k and top-p parameters, \u003cmark style=\"background: #D2B3FFA6;\"\u003echanging the temperature alters the predictions that the model will make. \u003c/mark\u003e\n\nfor a low value of temperature (\u003c1), the resulting probability distribution from the softmax layer is more strongly peaked with the probability being concentrated in a smaller number of words\n\n![[notes/images/Screenshot 2023-08-10 at 5.54.12 AM.png]]\n\nThe model will select from this distribution using random sampling as such resulting text will be less random, closely following the most likely word sequences that the model learned during training\n\nfor a high value of temperature (\u003e1), then the model will calculate a broader flatter probability distribution for the next token\n\n![[notes/images/Screenshot 2023-08-10 at 5.55.02 AM.png]]\n\nThis leads the model to generate text with a higher degree of randomness and more variability in the output. It can help you generate text that sounds more creative\n\nIf temperature value is equal to one, softmax function will behave as default and the unaltered probability distribution will be used. \n\n","lastmodified":"2023-08-25T15:19:54.874738913Z","tags":["rn"]},"/notes/20230810055842-Top-P-sampling":{"title":"20230810055842 Top-P sampling","content":"\n\u003c!-- \ntags: \n--\u003e\n\n\u003c!--internal\nparent:: [[202308100523 Generative Configuration]]\nchild:: [[]]\nrelated:: [[]]\n--\u003e\n\n\u003c!--external\n- [ ] []()\n--\u003e\n\n# Top-P sampling\n\nThe top p setting to limit the random sampling to the predictions whose combined probabilities do not exceed p. \n\nThe model then uses the random probability weighting method to choose from these tokens. \n\n![[notes/images/Screenshot 2023-08-10 at 5.52.50 AM.png]]\n","lastmodified":"2023-08-25T15:19:54.874738913Z","tags":["rn"]},"/notes/20230810055942-Top-K-sampling":{"title":"20230810055942 Top-K sampling","content":"\n\u003c!-- \ntags: \n--\u003e\n\n\u003c!--internal\nparent:: [[202308100523 Generative Configuration]]\nchild:: [[]]\nrelated:: [[]]\n--\u003e\n\n\u003c!--external\n- [ ] []()\n--\u003e\n\n# Top-K sampling\n\nThe top-k value instructs the model to choose from only the k tokens with the highest probabilities\n\nThis method can help the model have some randomness while preventing the selection of highly improbable completion words. In turn, text generated to sound more reasonable and sensible\n\n![[notes/images/Screenshot 2023-08-10 at 5.52.26 AM.png]]\n\n\n","lastmodified":"2023-08-25T15:19:54.874738913Z","tags":["rn"]},"/notes/20230810060024-Random-Sampling":{"title":"20230810060024 Random Sampling","content":"\n\u003c!-- \ntags: \n--\u003e\n\n\u003c!--internal\nparent:: [[202308100523 Generative Configuration]]\nchild:: [[]]\nrelated:: [[]]\n--\u003e\n\n\u003c!--external\n- [ ] []()\n--\u003e\n\n# Random Sampling\n\nInstead of selecting the most probable word every time with random sampling, the model chooses an output word at random using the probability distribution to weight the selection. \n\nIt reduces the likelihood that words will be repeated\n\nHowever, in some cases the output might end be too creative, producing words that cause the generation to wander off into topics or words that just don't make sense\n\ntop-p and top-k are sampling techniques used to limit the random sampling and increase the chance that the output will be sensible\n## Sampling Techniques\n\n[[notes/20230810055942 Top-K sampling|Top-K sampling]]\n\n[[notes/20230810055842 Top-P sampling|Top-P sampling]]\n\n","lastmodified":"2023-08-25T15:19:54.874738913Z","tags":["rn"]},"/notes/20230810060228-Greedy-Sampling":{"title":"20230810060228 Greedy Sampling","content":"\n\u003c!-- \ntags: \n--\u003e\n\n\u003c!--internal\nparent:: [[202308100523 Generative Configuration]]\nchild:: [[]]\nrelated:: [[]]\n--\u003e\n\n\u003c!--external\n- [ ] []()\n--\u003e\n\n# Greedy Sampling\n\nThis is the simplest form of next-word prediction, where the model will choose the word with the highest probability - most [[notes/20230628030810 Large Language Models (LLMs)|Large Language Models]] operate use greedy decodingf\n\nThis method can work quite well for short text generation but is susceptible to repeated words or repeated sequences of words. \n","lastmodified":"2023-08-25T15:19:54.874738913Z","tags":["rn"]},"/notes/202308100604-Sampling":{"title":"202308100604 Sampling","content":"\n\u003c!--\ntags: \n--\u003e\n\n\u003c!--internal\nparent:: [[]]\nchild:: [[]]\nrelated:: [[]]\n--\u003e\n\n\u003c!--external\n- [ ] []()\n--\u003e\n# Sampling\n\nThe output from the transformer's softmax layer is a probability distribution across the entire dictionary of words available to the model\n\n![[notes/images/Screenshot 2023-08-10 at 5.31.50 AM.png]]\n\n- [[notes/20230810060228 Greedy Sampling|Greedy Sampling]]\n- [[notes/20230810060024 Random Sampling|Random Sampling]]","lastmodified":"2023-08-25T15:19:54.874738913Z","tags":[""]},"/notes/202308100605-Max-New-Token":{"title":"202308100605 Max New Token","content":"\n\u003c!--\ntags: \n--\u003e\n\n\u003c!--internal\nparent:: [[]]\nchild:: [[]]\nrelated:: [[]]\n--\u003e\n\n\u003c!--external\n- [ ] []()\n--\u003e\n# Max New Token\n\nMax new tokens is used to limit the number of tokens that the model will generate. It is like putting a cap on the number of times the model will go through the selection process. ","lastmodified":"2023-08-25T15:19:54.874738913Z","tags":[""]},"/notes/20230810063410-Application-Integration-Stage":{"title":"20230810063410 Application Integration Stage","content":"\n\u003c!-- \ntags: \n--\u003e\n\n\u003c!--internal\nparent:: [[202308061649 Generative AI project Lifecycle]]\nchild:: [[]]\nrelated:: [[]]\n--\u003e\n\n\u003c!--external\n- [ ] []()\n--\u003e\n# Application Integration Stage\n\nwhen we've got model that meets the performance needs and aligned to the requirement, we can deploy it on our infrastructure and integrate it with our application\n\nAn important step is to optimize the model for deployment in order to best use of compute resources and providing the best possible experience for the users of the application","lastmodified":"2023-08-25T15:19:54.874738913Z","tags":["rn"]},"/notes/20230810063457-Adapt-and-Align-stage":{"title":"20230810063457 Adapt and Align stage","content":"\n\u003c!-- \ntags: \n--\u003e\n\n\u003c!--internal\nparent:: [[202308061649 Generative AI project Lifecycle]]\nchild:: [[]]\nrelated:: [[]]\n--\u003e\n\n\u003c!--external\n- [ ] []()\n--\u003e\n\n# Adapt and Align Stage\n\nThe next step is to assess the model's performance and perform additional training if needed\n\n[[notes/202308090519 Prompt Engineering|Prompt Engineering]] can sometimes be enough to get the model to perform well, start with trying in-context learning, using examples suited to the task and use case. \n\nThere are still cases, however, where the model might not perform as well as required, even with one or a few short inference, and in that case, we can try [[notes/202308061628 Instruction Fine Tuning|Fine-Tuning]] the model\n\nset up some metrics and benchmarks to determine how well the model is performing or how well it is aligned to the requirements and preferencs\n\nThe adapt and aligned stage can be quite iterative in order to get the performance required\n- attempt prompt engineering\n- evaluating the output\n- use fine tuning to improve performance\n- revisiting and evaluate prompt engineering one more time ","lastmodified":"2023-08-25T15:19:54.874738913Z","tags":["rn"]},"/notes/20230810063546-Select-Stage":{"title":"20230810063546 Select Stage","content":"\n\u003c!-- \ntags: \n--\u003e\n\n\u003c!--internal\nparent:: [[202308061649 Generative AI project Lifecycle]]\nchild:: [[]]\nrelated:: [[]]\n--\u003e\n\n\u003c!--external\n- [ ] []()\n--\u003e\n\n# Select Stage\n\nOne of the first things we have to decide is whether we're taking a [[notes/20230806171025 Foundational Models|Foundational Models]] off the shelf or pre-training our own model\n\nIn general, we'll start with an existing model, although there are some cases where it might it required to train a model from scratch. \n\n\u003e when do you need a giant model, 100 billion (or bigger) parameters\n\u003e\n\u003e and when can a 1-30 billion parameter model or\n\u003e\n\u003e or even sub 1 billion parameter model made to work for a specific application?\n\n\u003e we can use quite small models and still get quite a lot of capability out of them","lastmodified":"2023-08-25T15:19:54.874738913Z","tags":["rn"]},"/notes/20230810063625-Scope-Stage":{"title":"20230810063625 Scope Stage","content":"\n\u003c!-- \ntags: \n--\u003e\n\n\u003c!--internal\nparent:: [[202308061649 Generative AI project Lifecycle]]\nchild:: [[]]\nrelated:: [[]]\n--\u003e\n\n\u003c!--external\n- [ ] []()\n--\u003e\n\n# Scope Stage\n\nThe most important step in the project is to define the scope as accurately and narrowly as possible\n\n[[notes/20230628030810 Large Language Models (LLMs)|LLMs]] are capable of carrying out several tasks, but their abilities depend on the size and architecture of the model. \n\nThink about what function the LLM will have in the specific application. Do we need the model to be able to carry out many different tasks, including long-form text generation or is the task much more specific like named entity recognition so that the model needs to be good at one thing alone\n\nBeing specific about what we need the model to do can save time and perhaps more important compute cost\n\n","lastmodified":"2023-08-25T15:19:54.874738913Z","tags":["rn"]},"/notes/Indegene":{"title":"Indegene","content":"\n\u003c!-- \ntags: \n--\u003e\n\n\u003c!--internal\nparent:: [[]]\nchild:: [[]]\nrelated:: [[]]\n--\u003e\n\n\u003c!--external\n- []()\n--\u003e\n\n# Indegene\n\n[[notes/Index|Generative AI]]\n## Usecases\n\n- [[notes/20230711082511 Campaign Creation to Tactic Authoring|Campaign Creation to Tactic Authoring]]\n\t- OpenAI API\n- [[notes/20230711101408 Content Repurposing (Email to Video)|Content Repurposing (Email to Video)]]\n- Semantic Image Search\n\t- OpenAI API for Embedding Images\n\t- pgVector\n- Semantic QnA\n\t- OpenAI API for Embedding Text\n\t- pgVector\n\t- Langchain\n","lastmodified":"2023-08-25T15:19:54.874738913Z","tags":["InProgress","project"]}}