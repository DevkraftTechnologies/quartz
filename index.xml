<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Devkraft Technologies on</title><link>https://quartz.devkraft.in/</link><description>Recent content in Devkraft Technologies on</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="https://quartz.devkraft.in/index.xml" rel="self" type="application/rss+xml"/><item><title/><link>https://quartz.devkraft.in/notes/202308051230-Generative-AI-with-Large-Language-Models/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/202308051230-Generative-AI-with-Large-Language-Models/</guid><description>Generative AI with Large Language Models Objectives Discuss model pre-training and the value of continued pre-training vs fine-tuning Define the terms [[notes/20230628030901 Generative AI|Generative AI]], [[notes/20230628030810 Large Language Models (LLMs)|Large Language Models]], prompt, and describe the [[notes/20230703011428 Transformers|Transformer]] architecture that powers LLMs Describe the steps in a typical LLM-based, generative AI model lifecycle and discuss the constraining factors that drive decisions at each step of model lifecycle Discuss computational challenges during model pre-training and determine how to efficiently reduce memory footprint Define the term scaling law and describe the laws that have been discovered for LLMs related to training dataset size, compute budget, inference requirements, and other factors.</description></item><item><title/><link>https://quartz.devkraft.in/notes/202308051314-Reinforced-Learning-from-Human-Feedback-RLHF/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/202308051314-Reinforced-Learning-from-Human-Feedback-RLHF/</guid><description>Reinforced Learning from Human Feedback (RLHF)</description></item><item><title/><link>https://quartz.devkraft.in/notes/202308061603-Research-Papers/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/202308061603-Research-Papers/</guid><description>Research Papers [[notes/202308090446 Attention is All You Need|Attention is All You Need]] It allowed attention to work in parallel manner at a massive scale</description></item><item><title/><link>https://quartz.devkraft.in/notes/202308061627-In-context-Learning/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/202308061627-In-context-Learning/</guid><description>In-context Learning</description></item><item><title/><link>https://quartz.devkraft.in/notes/202308061628-Instruction-Fine-Tuning/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/202308061628-Instruction-Fine-Tuning/</guid><description>Instruction Fine Tuning [[notes/202308051314 Reinforced Learning from Human Feedback (RLHF)|RLHF]]</description></item><item><title/><link>https://quartz.devkraft.in/notes/202308061649-Generative-AI-project-Lifecycle/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/202308061649-Generative-AI-project-Lifecycle/</guid><description>Generative AI project Life Cycle Build a good intuition about the important decisions to make, the potential difficulties, and the infrastructure needed to develop and serve the application</description></item><item><title/><link>https://quartz.devkraft.in/notes/20230806171025-Foundational-Models/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/20230806171025-Foundational-Models/</guid><description>Foundational Models The choice is whether to use Foundational [[notes/20230628030810 Large Language Models (LLMs)|LLMs]] as it is or perform instruction fine tuning to tailor it to a particular task</description></item><item><title/><link>https://quartz.devkraft.in/notes/20230807051721-Transformer-Architecture/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/20230807051721-Transformer-Architecture/</guid><description>Transformer Architecture ![[notes/images/Screenshot 2023-07-03 at 1.28.32 AM.png|250]]
The [[notes/20230703011428 Transformers|Transformer]] architecture is split into two distinct parts, the [[notes/202308090432 Encoder|Encoder]] and the [[notes/202308090433 Decoder|Decoder]] !</description></item><item><title/><link>https://quartz.devkraft.in/notes/202308090324-Generating-Text-with-Transformers/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/202308090324-Generating-Text-with-Transformers/</guid><description>Generating Text with Transformers The complete [[notes/20230807051721 Transformer Architecture|Transformer Architecture]] consists of an [[notes/202308090432 Encoder|Encoder]] and [[notes/202308090433 Decoder|Decoder]] components
How does overall prediction process works from end to end for a translation (or sequence-to-sequence) task</description></item><item><title/><link>https://quartz.devkraft.in/notes/20230809040949-Decoder-only-Models/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/20230809040949-Decoder-only-Models/</guid><description>Decoder-only Models ![[notes/images/Screenshot 2023-08-09 at 4.44.22 AM.png|250]]
How do [[notes/202308090433 Decoder|Decoder]]-only models (like GPT, BLOOM, Jurassic, LLaMA) generalize to most tasks with scale #TODO</description></item><item><title/><link>https://quartz.devkraft.in/notes/20230809041643-Encoder-Decoder-Models/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/20230809041643-Encoder-Decoder-Models/</guid><description>Encoder-Decoder Models Encoder-decoder models (like BART) perform well on sequence-to-sequence tasks such as translation, where the input sequence and the output sequence can be different lengths.</description></item><item><title/><link>https://quartz.devkraft.in/notes/20230809041717-Encoder-only-Models/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/20230809041717-Encoder-only-Models/</guid><description>Encoder-only ModelsX [[notes/202308090432 Encoder|Encoder]]-only models (like BERT) also work as sequence-to-sequence models where the input sequence and the output sequence are of the same length With additional layers to the architecture, It is possible to train encoder-only models to perform classification tasks such as sentiment !</description></item><item><title/><link>https://quartz.devkraft.in/notes/202308090432-Encoder/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/202308090432-Encoder/</guid><description>Encoder The encoder encodes input sequences into a deep representation of the structure and meaning of the input</description></item><item><title/><link>https://quartz.devkraft.in/notes/202308090433-Decoder/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/202308090433-Decoder/</guid><description>Decoder The decoder, working from input token triggers, uses the encoder&amp;rsquo;s contextual understanding to generate new tokens. It does this in a loop until some stop condition has been reached.</description></item><item><title/><link>https://quartz.devkraft.in/notes/202308090446-Attention-is-All-You-Need/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/202308090446-Attention-is-All-You-Need/</guid><description>Attention Is All You Need ![[notes/images/Pasted image 20230809044742.png]]
The Transformer architecture consists of an encoder and a decoder, each of which is composed of several layers</description></item><item><title/><link>https://quartz.devkraft.in/notes/202308090519-Prompt-Engineering/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/202308090519-Prompt-Engineering/</guid><description>Prompt Engineering Prompts are the text that we feed into the model. The act of generating text is known as Inference and the output text is known as the completion</description></item><item><title/><link>https://quartz.devkraft.in/notes/202308090550-Context-Window/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/202308090550-Context-Window/</guid><description>Context Window The full amount of text or the memory available to use for the prompt is called the Context Window.</description></item><item><title/><link>https://quartz.devkraft.in/notes/20230809055132-In-Context-Learning-ICL/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/20230809055132-In-Context-Learning-ICL/</guid><description>In-Context Learning (ICL) In-Context Learning is the practise of providing examples inside the [[notes/202308090550 Context Window|Context Window]] of a Prompt in order to get the model to produce better outcomes.</description></item><item><title/><link>https://quartz.devkraft.in/notes/20230809055214-Few-Shot-Inference/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/20230809055214-Few-Shot-Inference/</guid><description>Few-Shot Inference The inclusion of a multiple examples is known as few-shot inference. A mix of examples with different output classes is preferred</description></item><item><title/><link>https://quartz.devkraft.in/notes/20230809055340-One-Shot-Inference/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/20230809055340-One-Shot-Inference/</guid><description>One-Shot Inference ![[notes/images/Screenshot 2023-08-09 at 5.39.09 AM.png]]
The inclusion of a single example is known as one-shot inference</description></item><item><title/><link>https://quartz.devkraft.in/notes/20230809055419-Zero-Shot-Inference/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/20230809055419-Zero-Shot-Inference/</guid><description>Zero-Shot Inference ![[notes/images/Screenshot 2023-08-09 at 5.34.39 AM.png]]
The prompt consists of the instruction, &amp;ldquo;classify this review,&amp;rdquo; followed by some context in this case is the review text itself, and an instruction to produce the sentiment at the end</description></item><item><title/><link>https://quartz.devkraft.in/notes/202308100523-Generative-Configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/202308100523-Generative-Configuration/</guid><description>Generative Configuration Each [[notes/20230628030901 Generative AI|Generative]] model exposes a set of configuration parameters that can influence the model&amp;rsquo;s output during inference.</description></item><item><title/><link>https://quartz.devkraft.in/notes/20230810055747-Temperature/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/20230810055747-Temperature/</guid><description>Temperature The temperature parameter influences the shape of the probability distribution that the model calculates for the next token
The higher the temperature, the higher the randomness and the lower the temperature, the lower the randomness.</description></item><item><title/><link>https://quartz.devkraft.in/notes/20230810055842-Top-P-sampling/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/20230810055842-Top-P-sampling/</guid><description>Top-P sampling The top p setting to limit the random sampling to the predictions whose combined probabilities do not exceed p.</description></item><item><title/><link>https://quartz.devkraft.in/notes/20230810055942-Top-K-sampling/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/20230810055942-Top-K-sampling/</guid><description>Top-K sampling The top-k value instructs the model to choose from only the k tokens with the highest probabilities
This method can help the model have some randomness while preventing the selection of highly improbable completion words.</description></item><item><title/><link>https://quartz.devkraft.in/notes/20230810060024-Random-Sampling/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/20230810060024-Random-Sampling/</guid><description>Random Sampling Instead of selecting the most probable word every time with random sampling, the model chooses an output word at random using the probability distribution to weight the selection.</description></item><item><title/><link>https://quartz.devkraft.in/notes/20230810060228-Greedy-Sampling/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/20230810060228-Greedy-Sampling/</guid><description>Greedy Sampling This is the simplest form of next-word prediction, where the model will choose the word with the highest probability - most [[notes/20230628030810 Large Language Models (LLMs)|Large Language Models]] operate use greedy decodingf</description></item><item><title/><link>https://quartz.devkraft.in/notes/202308100604-Sampling/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/202308100604-Sampling/</guid><description>Sampling The output from the transformer&amp;rsquo;s softmax layer is a probability distribution across the entire dictionary of words available to the model</description></item><item><title/><link>https://quartz.devkraft.in/notes/202308100605-Max-New-Token/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/202308100605-Max-New-Token/</guid><description>Max New Token Max new tokens is used to limit the number of tokens that the model will generate. It is like putting a cap on the number of times the model will go through the selection process.</description></item><item><title/><link>https://quartz.devkraft.in/notes/20230810063410-Application-Integration-Stage/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/20230810063410-Application-Integration-Stage/</guid><description>Application Integration Stage when we&amp;rsquo;ve got model that meets the performance needs and aligned to the requirement, we can deploy it on our infrastructure and integrate it with our application</description></item><item><title/><link>https://quartz.devkraft.in/notes/20230810063457-Adapt-and-Align-stage/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/20230810063457-Adapt-and-Align-stage/</guid><description>Adapt and Align Stage The next step is to assess the model&amp;rsquo;s performance and perform additional training if needed
[[notes/202308090519 Prompt Engineering|Prompt Engineering]] can sometimes be enough to get the model to perform well, start with trying in-context learning, using examples suited to the task and use case.</description></item><item><title/><link>https://quartz.devkraft.in/notes/20230810063546-Select-Stage/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/20230810063546-Select-Stage/</guid><description>Select Stage One of the first things we have to decide is whether we&amp;rsquo;re taking a [[notes/20230806171025 Foundational Models|Foundational Models]] off the shelf or pre-training our own model</description></item><item><title/><link>https://quartz.devkraft.in/notes/20230810063625-Scope-Stage/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/20230810063625-Scope-Stage/</guid><description>Scope Stage The most important step in the project is to define the scope as accurately and narrowly as possible</description></item><item><title>Add a button to product page to share the product</title><link>https://quartz.devkraft.in/notes/20230702210325-Add-a-button-to-product-page-to-share-the-product/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/20230702210325-Add-a-button-to-product-page-to-share-the-product/</guid><description>Add a button to product page to share the product From a technical standpoint, implementing this change (i.e. adding this button) can take a bit of time</description></item><item><title>Agents</title><link>https://quartz.devkraft.in/notes/20230703063700-Agents/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/20230703063700-Agents/</guid><description>Agents Agents are a method of using a language model as a reasoning engine to determine how to interact with the outside world based on the user&amp;rsquo;s input.</description></item><item><title>Alternatives to LangChain</title><link>https://quartz.devkraft.in/notes/20230703063839-Alternatives-to-LangChain/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/20230703063839-Alternatives-to-LangChain/</guid><description>Alternatives to [[notes/20230703061520 LangChain|LangChain]] AutoGPT Auto-GPT has tendencies where it gets stuck in infinite logic loops and rabbit holes AgentGPT BabyAGI LangDock GradientJ FlowiseAI TensorFlow</description></item><item><title>Attention</title><link>https://quartz.devkraft.in/notes/20230703013343-Attention/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/20230703013343-Attention/</guid><description>Attention Attention allows the model to focus on important parts of the input, such as words in a sentence or features in an image</description></item><item><title>Build GPT from scratch</title><link>https://quartz.devkraft.in/notes/20230628030911-Build-GPT-from-scratch/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/20230628030911-Build-GPT-from-scratch/</guid><description>Build GPT from scratch watch andrej karpathy builds [[notes/20230628031147 Generative Pretrained Transformer (GPT)|GPT]] from Scratch in his video</description></item><item><title>Build Your Own</title><link>https://quartz.devkraft.in/notes/20230703060600-Build-Your-Own/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/20230703060600-Build-Your-Own/</guid><description>Build Your Own [[notes/20230628031147 Generative Pretrained Transformer (GPT)|GPT]] nanoGPT miniGPT [[notes/20230703043154 Vector Database|Vector Database]] Vector Databases Demystified: Part 2 - Building Your Own (Very) Simple Vector Database in Python</description></item><item><title>ChatGPT</title><link>https://quartz.devkraft.in/notes/20230630042526-ChatGPT/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/20230630042526-ChatGPT/</guid><description>ChatGPT ChatGPT uses [[notes/20230628031546 Generative Adversarial Networks (GANs)|GANs]] to generate responses to input text, allowing it to engage in natural-sounding conversations with humans</description></item><item><title>Checklist</title><link>https://quartz.devkraft.in/notes/20230628030800-Checklist/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/20230628030800-Checklist/</guid><description>Checklist what would be genAI first experience would be for users in an ideal scenario.
How [[notes/20230628030901 Generative AI|Generative AI]] works APIs of OpenAI Azure versions of [[notes/20230630042526 ChatGPT|ChatGPT]] How to send customer data to such APIs How to to keep customer data private [[notes/20230703043154 Vector Database|Vector Database]]/[[notes/20230703031649 Word Embedding|Embedding]] How to improve accuracy and reduce hallucinations How to serve structured data back to customer in consistent manner (optional) Other LLMs (optional) Chaining results from different LLMs [[notes/20230703061520 LangChain|LangChain]] How to experiment with and provide this to users at low cost</description></item><item><title>Conditional Distribution</title><link>https://quartz.devkraft.in/notes/20230630021015-Conditional-Distribution/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/20230630021015-Conditional-Distribution/</guid><description>Conditional Distribution A conditional distribution provides the probability of A occurring given B
For example, we can ask what the probability of rolling a specific number on a 6 sided die is.</description></item><item><title>Discriminative AI</title><link>https://quartz.devkraft.in/notes/20230630020251-Discriminative-AI/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/20230630020251-Discriminative-AI/</guid><description>Discriminative AI Discriminative AI is useful when we want to make some sort of decision
For example, we might want to predict whether someone is at risk for cancer given some biometric data - height, weight, blood pressure, etc.</description></item><item><title>Generative Adversarial Networks (GANs)</title><link>https://quartz.devkraft.in/notes/20230628031546-Generative-Adversarial-Networks-GANs/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/20230628031546-Generative-Adversarial-Networks-GANs/</guid><description>Generative Adversarial Networks (GANs) Generative Adversarial Networks or GANs are a type of neural network that uses two competing networks - a generator and a discriminator</description></item><item><title>Generative AI</title><link>https://quartz.devkraft.in/notes/20230628030901-Generative-AI/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/20230628030901-Generative-AI/</guid><description>Generative AI Generative AI refers to Artificial Intelligence models that generates novel data, information, or documents</description></item><item><title>Generative AI - A Mathematical Perspective</title><link>https://quartz.devkraft.in/notes/20230630041814-Generative-AI-A-Mathematical-Perspective/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/20230630041814-Generative-AI-A-Mathematical-Perspective/</guid><description>Generative AI - A Mathematical Perspective Generative AI is modeling a [[notes/20230630021029 Joint Distribution|Joint Distribution]] because the distribution itself is the object of interest</description></item><item><title>Generative AI vs Discriminative AI</title><link>https://quartz.devkraft.in/notes/20230630020526-Generative-AI-vs-Discriminative-AI/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/20230630020526-Generative-AI-vs-Discriminative-AI/</guid><description>Generative AI vs Discriminative AI When working with [[notes/20230630020251 Discriminative AI|Discriminative AI]], we don’t care about these features in and of themselves - we only care about them insofar as they help us make a decision.</description></item><item><title>Generative AI vs Discriminative AI - A mathematical perspective</title><link>https://quartz.devkraft.in/notes/20230630020526-Generative-AI-vs-Discriminative-AI-A-mathematical-perspective/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/20230630020526-Generative-AI-vs-Discriminative-AI-A-mathematical-perspective/</guid><description>Generative AI vs Discriminative AI - A Mathematical Perspective Discriminative AI is considered to be modeling a [[notes/20230630021015 Conditional Distribution|Conditional Distribution]] whereas Generative AI is considered to be modeling a [[notes/20230630021029 Joint Distribution|Joint Distribution]]</description></item><item><title>Generative Pretrained Transformer (GPT)</title><link>https://quartz.devkraft.in/notes/20230628031147-Generative-Pretrained-Transformer-GPT/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/20230628031147-Generative-Pretrained-Transformer-GPT/</guid><description>Generative Pretrained Transformer (GPT) GPT stands for Generative Pretrained [[notes/20230703011428 Transformers|Transformer]]</description></item><item><title>GPT-3</title><link>https://quartz.devkraft.in/notes/20230628033256-GPT-3/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/20230628033256-GPT-3/</guid><description>GPT-3 The architecture is a transformer decoder model [[notes/20230628031147 Generative Pretrained Transformer (GPT)|GPT]]-3 works is novel due to it&amp;rsquo;s huge scale.</description></item><item><title>How does GPT work</title><link>https://quartz.devkraft.in/notes/20230628032202-How-does-GPT-work/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/20230628032202-How-does-GPT-work/</guid><description>How does [[notes/20230628031147 Generative Pretrained Transformer (GPT)|GPT]] work</description></item><item><title>How to use Generative AI</title><link>https://quartz.devkraft.in/notes/20230630042902-How-to-use-Generative-AI/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/20230630042902-How-to-use-Generative-AI/</guid><description>How to use Generative AI The most general way to think about [[notes/20230628030901 Generative AI|Generative AI]] is as a mapping from (potential) antecedents to desired consequents</description></item><item><title>Joint Distribution</title><link>https://quartz.devkraft.in/notes/20230630021029-Joint-Distribution/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/20230630021029-Joint-Distribution/</guid><description>Joint Distribution A joint distribution provides the probability of A occurring alongside B
For example, what is the probability of rolling a 2 on a first die roll and a 3 on a second die roll?</description></item><item><title>LangChain</title><link>https://quartz.devkraft.in/notes/20230703061520-LangChain/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/20230703061520-LangChain/</guid><description>LangChain LangChain is a framework for developing applications powered by language models
It can call a language model via an API It can also connect [[notes/20230628030810 Large Language Models (LLMs)|LLMs]], like GPT-4, LLaMDA, and LLaMA, to other sources of data such as Google Drive, Wikipedia and allow them to interact with them It can chain commands together so the AI model can know what it needs to do to produce the results or perform the actions needed Features !</description></item><item><title>Large Language Models (LLMs)</title><link>https://quartz.devkraft.in/notes/20230628030810-Large-Language-Models-LLMs/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/20230628030810-Large-Language-Models-LLMs/</guid><description>Large Language Model (LLMs) The parameters a like a model&amp;rsquo;s memory, the more parameters a model has the more sophisticated task it can perform</description></item><item><title>Links</title><link>https://quartz.devkraft.in/notes/20230628030801-Links/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/20230628030801-Links/</guid><description>Links Vinija Jain models Neural Networks: Zero to Hero Awesome-LLM Awesome-LLM-Large-Language-Models-Notes BLOOM Is the Most Important AI Model of the Decade What Is ChatGPT Doing … and Why Does It Work?</description></item><item><title>LSTMs</title><link>https://quartz.devkraft.in/notes/20230703012514-LSTMs/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/20230703012514-LSTMs/</guid><description>LSTMs LSTMs unlike [[notes/20230703012211 Recurrent Neural Networks (RNNs)|RNNs]] can remember But LSTMs take too long to train</description></item><item><title>Memory</title><link>https://quartz.devkraft.in/notes/20230703063746-Memory/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/20230703063746-Memory/</guid><description>Memory LLMs process each query independently of other interactions.
LangChain provides memory components to manage and manipulate previous chat messages and incorporate them into chains.</description></item><item><title>MidJourneyAI</title><link>https://quartz.devkraft.in/notes/20230711171651-MidJourneyAI/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/20230711171651-MidJourneyAI/</guid><description>MidJourney AI annual subscription cost for basic plan cost $96
Tips how to change the aspect ratio of images include “—ar 3:2” in the prompt include “full body” if you don’t want super close-ups add term “cinematic,” to prompts add specific dates for reference.</description></item><item><title>Modern Generative AI</title><link>https://quartz.devkraft.in/notes/20230702204625-Modern-Generative-AI/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/20230702204625-Modern-Generative-AI/</guid><description>Modern Generative AI [[notes/20230628030901 Generative AI|Generative AI]] is not a new tech, but the recent explosion in performance and interest can be attributed to advances made in the last 5 years</description></item><item><title>Normalization</title><link>https://quartz.devkraft.in/notes/20230703034916-Normalization/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/20230703034916-Normalization/</guid><description>Normalization Normalization, in the context of machine learning, refers to the process of standardizing or scaling data to a common range or distribution.</description></item><item><title>PgVector</title><link>https://quartz.devkraft.in/notes/20230703070807-PgVector/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/20230703070807-PgVector/</guid><description>PgVector PgVector supports up to 16,000 dimensions for storage and 2,000 for indexing PgVector supports exact KNN which is what most apps just starting their journey (&amp;lt;10k docs) need It supports IVF ANN for larger collections postgres natively supports several additional composite index types, including BTREE GIST and GIN, for tabular, text and json data in addition to vector indexes.</description></item><item><title>Positional Encoding</title><link>https://quartz.devkraft.in/notes/20230703031923-Positional-Encoding/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/20230703031923-Positional-Encoding/</guid><description>Positional Encoding Positional Encoding is used to provide information about the position of words within a sentence.
Consider the sentence: &amp;ldquo;The cat sat on the mat.</description></item><item><title>PostgresML</title><link>https://quartz.devkraft.in/notes/20230703073417-PostgresML/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/20230703073417-PostgresML/</guid><description>PostgresML</description></item><item><title>Recurrent Neural Networks (RNNs)</title><link>https://quartz.devkraft.in/notes/20230703012211-Recurrent-Neural-Networks-RNNs/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/20230703012211-Recurrent-Neural-Networks-RNNs/</guid><description>Recurrent Neural Networks (RNNs) ![[notes/images/Screenshot 2023-08-07 at 4.53.07 AM.png]]
Limitations with just one previous word available to the model, prediction was not good</description></item><item><title>Transformers</title><link>https://quartz.devkraft.in/notes/20230703011428-Transformers/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/20230703011428-Transformers/</guid><description>Transformers Overview Transformers were introduced in [[notes/202308090446 Attention is All You Need|Attention is All You Need]] The power of the transformer architecture lies in its ability to learn the relevance and context not just of each word next to its neighbor !</description></item><item><title>Usecases</title><link>https://quartz.devkraft.in/notes/20230703150959-Usecases/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/20230703150959-Usecases/</guid><description>Usecases Generative AI for concept arts tinder like interface for human review/feedback track line of sight to build heatmaps of attention (reminds me of the paper spandan worked on)</description></item><item><title>Vector Database</title><link>https://quartz.devkraft.in/notes/20230703043154-Vector-Database/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/20230703043154-Vector-Database/</guid><description>Vector Database These databases contain arrays of numbers clustered together based on similarities, which can be queried with ultra-low latencies</description></item><item><title>Word Embedding</title><link>https://quartz.devkraft.in/notes/20230703031649-Word-Embedding/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/20230703031649-Word-Embedding/</guid><description>Word Embedding Word Embeddings are vector representations of words that capture semantic and syntactic relationships between words
consider the following words and respective word embeddings in a 3-dimensional space:</description></item></channel></rss>