<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>TODO on</title><link>https://quartz.devkraft.in/tags/TODO/</link><description>Recent content in TODO on</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="https://quartz.devkraft.in/tags/TODO/index.xml" rel="self" type="application/rss+xml"/><item><title>Attention</title><link>https://quartz.devkraft.in/notes/20230703013343-Attention/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/20230703013343-Attention/</guid><description>Attention Attention allows the model to focus on important parts of the input, such as words in a sentence or features in an image</description></item><item><title>Build GPT from scratch</title><link>https://quartz.devkraft.in/notes/20230628030911-Build-GPT-from-scratch/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/20230628030911-Build-GPT-from-scratch/</guid><description>Let&amp;rsquo;s build GPT: from scratch, in code, spelled out.</description></item><item><title>Generative Adversarial Networks (GANs)</title><link>https://quartz.devkraft.in/notes/20230628031546-Generative-Adversarial-Networks-GANs/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/20230628031546-Generative-Adversarial-Networks-GANs/</guid><description>Generative Adversarial Networks (GANs) Generative Adversarial Networks or GANs are a type of neural network that uses two competing networks - a generator and a discriminator</description></item><item><title>Generative Pretrained Transformer (GPT)</title><link>https://quartz.devkraft.in/notes/20230628031147-Generative-Pretrained-Transformer-GPT/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/20230628031147-Generative-Pretrained-Transformer-GPT/</guid><description>Generative Pretrained Transformer (GPT) GPT stands for Generative Pretrained [[notes/20230703011428 Transformers|Transformer]]</description></item><item><title>GPT-3</title><link>https://quartz.devkraft.in/notes/20230628033256-GPT-3/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/20230628033256-GPT-3/</guid><description>GPT-3 The architecture is a transformer decoder model
GPT-3 works is novel due to it&amp;rsquo;s huge scale. GPT3 is MASSIVE. It encodes what it learns from training in 175 billion numbers (called parameters).</description></item><item><title>How does GPT work</title><link>https://quartz.devkraft.in/notes/20230628032202-How-does-GPT-work/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/20230628032202-How-does-GPT-work/</guid><description>How does GPT work</description></item><item><title>How to use Generative AI</title><link>https://quartz.devkraft.in/notes/20230630042902-How-to-use-Generative-AI/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/20230630042902-How-to-use-Generative-AI/</guid><description>How to use Generative AI The most general way to think about [[notes/20230628030901 Generative AI|Generative AI]] is as a mapping from (potential) antecedents to desired consequents</description></item><item><title>Large Language Models (LLMs)</title><link>https://quartz.devkraft.in/notes/20230628030810-Large-Language-Models-LLMs/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/20230628030810-Large-Language-Models-LLMs/</guid><description>Large Language Model (LLMs)</description></item><item><title>Links</title><link>https://quartz.devkraft.in/notes/20230628030801-Links/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/20230628030801-Links/</guid><description>Links Vinija Jain models Neural Networks: Zero to Hero Awesome-LLM Awesome-LLM-Large-Language-Models-Notes BLOOM Is the Most Important AI Model of the Decade What Is ChatGPT Doing â€¦ and Why Does It Work?</description></item><item><title>LSTMs</title><link>https://quartz.devkraft.in/notes/20230703012514-LSTMs/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/20230703012514-LSTMs/</guid><description>LSTMs LSTMs unlike [[notes/20230703012211 Recurrent Neural Networks (RNNs)|RNNs]] can remember for some time But LSTMs take too long to train</description></item><item><title>PostgresML</title><link>https://quartz.devkraft.in/notes/20230703073417-PostgresML/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/20230703073417-PostgresML/</guid><description>PostgresML</description></item><item><title>Recurrent Neural Networks (RNNs)</title><link>https://quartz.devkraft.in/notes/20230703012211-Recurrent-Neural-Networks-RNNs/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/20230703012211-Recurrent-Neural-Networks-RNNs/</guid><description>Recurrent Neural Networks (RNNs) when provided a sentence RNNs cannot remember the start of a sentence RNNs use recurrence as result cannot be parallelized</description></item><item><title>Transformers</title><link>https://quartz.devkraft.in/notes/20230703011428-Transformers/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/20230703011428-Transformers/</guid><description>Transformers ![[notes/images/Screenshot 2023-07-03 at 1.28.32 AM.png|250]]
Transformers use [[notes/20230703013343 Attention|Attention]] mechanisms instead of [[notes/20230703012211 Recurrent Neural Networks (RNNs)|Recurrent Neural Networks (RNNs)]] to remember information, making them faster and parallelizable.</description></item><item><title>Vector Database</title><link>https://quartz.devkraft.in/notes/20230703043154-Vector-Database/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/20230703043154-Vector-Database/</guid><description>Vector Database These databases contain arrays of numbers clustered together based on similarities, which can be queried with ultra-low latencies</description></item></channel></rss>