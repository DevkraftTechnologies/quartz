<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>rn on</title><link>https://quartz.devkraft.in/tags/rn/</link><description>Recent content in rn on</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="https://quartz.devkraft.in/tags/rn/index.xml" rel="self" type="application/rss+xml"/><item><title/><link>https://quartz.devkraft.in/notes/20230806171025-Foundational-Models/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/20230806171025-Foundational-Models/</guid><description>Foundational Models The choice is whether to use Foundational [[notes/20230628030810 Large Language Models (LLMs)|LLMs]] as it is or perform instruction fine tuning to tailor it to a particular task</description></item><item><title/><link>https://quartz.devkraft.in/notes/20230807051721-Transformer-Architecture/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/20230807051721-Transformer-Architecture/</guid><description>Transformer Architecture ![[notes/images/Screenshot 2023-07-03 at 1.28.32 AM.png|250]]
The [[notes/20230703011428 Transformers|Transformer]] architecture is split into two distinct parts, the [[notes/202308090432 Encoder|Encoder]] and the [[notes/202308090433 Decoder|Decoder]] !</description></item><item><title/><link>https://quartz.devkraft.in/notes/20230809040949-Decoder-only-Models/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/20230809040949-Decoder-only-Models/</guid><description>Decoder-only Models ![[notes/images/Screenshot 2023-08-09 at 4.44.22 AM.png|250]]
How do [[notes/202308090433 Decoder|Decoder]]-only models (like GPT, BLOOM, Jurassic, LLaMA) generalize to most tasks with scale #TODO</description></item><item><title/><link>https://quartz.devkraft.in/notes/20230809041643-Encoder-Decoder-Models/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/20230809041643-Encoder-Decoder-Models/</guid><description>Encoder-Decoder Models Encoder-decoder models (like BART) perform well on sequence-to-sequence tasks such as translation, where the input sequence and the output sequence can be different lengths.</description></item><item><title/><link>https://quartz.devkraft.in/notes/20230809041717-Encoder-only-Models/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/20230809041717-Encoder-only-Models/</guid><description>Encoder-only ModelsX [[notes/202308090432 Encoder|Encoder]]-only models (like BERT) also work as sequence-to-sequence models where the input sequence and the output sequence are of the same length With additional layers to the architecture, It is possible to train encoder-only models to perform classification tasks such as sentiment !</description></item><item><title/><link>https://quartz.devkraft.in/notes/202308090446-Attention-is-All-You-Need/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/202308090446-Attention-is-All-You-Need/</guid><description>Attention Is All You Need ![[notes/images/Pasted image 20230809044742.png]]
The Transformer architecture consists of an encoder and a decoder, each of which is composed of several layers</description></item><item><title/><link>https://quartz.devkraft.in/notes/20230809055132-In-Context-Learning-ICL/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/20230809055132-In-Context-Learning-ICL/</guid><description>In-Context Learning (ICL) In-Context Learning is the practise of providing examples inside the [[notes/202308090550 Context Window|Context Window]] of a Prompt in order to get the model to produce better outcomes.</description></item><item><title/><link>https://quartz.devkraft.in/notes/20230809055214-Few-Shot-Inference/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/20230809055214-Few-Shot-Inference/</guid><description>Few-Shot Inference The inclusion of a multiple examples is known as few-shot inference. A mix of examples with different output classes is preferred</description></item><item><title/><link>https://quartz.devkraft.in/notes/20230809055340-One-Shot-Inference/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/20230809055340-One-Shot-Inference/</guid><description>One-Shot Inference ![[notes/images/Screenshot 2023-08-09 at 5.39.09 AM.png]]
The inclusion of a single example is known as one-shot inference</description></item><item><title/><link>https://quartz.devkraft.in/notes/20230809055419-Zero-Shot-Inference/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/20230809055419-Zero-Shot-Inference/</guid><description>Zero-Shot Inference ![[notes/images/Screenshot 2023-08-09 at 5.34.39 AM.png]]
The prompt consists of the instruction, &amp;ldquo;classify this review,&amp;rdquo; followed by some context in this case is the review text itself, and an instruction to produce the sentiment at the end</description></item><item><title/><link>https://quartz.devkraft.in/notes/20230810055747-Temperature/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/20230810055747-Temperature/</guid><description>Temperature The temperature parameter influences the shape of the probability distribution that the model calculates for the next token
The higher the temperature, the higher the randomness and the lower the temperature, the lower the randomness.</description></item><item><title/><link>https://quartz.devkraft.in/notes/20230810055842-Top-P-sampling/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/20230810055842-Top-P-sampling/</guid><description>Top-P sampling The top p setting to limit the random sampling to the predictions whose combined probabilities do not exceed p.</description></item><item><title/><link>https://quartz.devkraft.in/notes/20230810055942-Top-K-sampling/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/20230810055942-Top-K-sampling/</guid><description>Top-K sampling The top-k value instructs the model to choose from only the k tokens with the highest probabilities
This method can help the model have some randomness while preventing the selection of highly improbable completion words.</description></item><item><title/><link>https://quartz.devkraft.in/notes/20230810060024-Random-Sampling/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/20230810060024-Random-Sampling/</guid><description>Random Sampling Instead of selecting the most probable word every time with random sampling, the model chooses an output word at random using the probability distribution to weight the selection.</description></item><item><title/><link>https://quartz.devkraft.in/notes/20230810060228-Greedy-Sampling/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/20230810060228-Greedy-Sampling/</guid><description>Greedy Sampling This is the simplest form of next-word prediction, where the model will choose the word with the highest probability - most [[notes/20230628030810 Large Language Models (LLMs)|Large Language Models]] operate use greedy decodingf</description></item><item><title/><link>https://quartz.devkraft.in/notes/20230810063410-Application-Integration-Stage/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/20230810063410-Application-Integration-Stage/</guid><description>Application Integration Stage when we&amp;rsquo;ve got model that meets the performance needs and aligned to the requirement, we can deploy it on our infrastructure and integrate it with our application</description></item><item><title/><link>https://quartz.devkraft.in/notes/20230810063457-Adapt-and-Align-stage/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/20230810063457-Adapt-and-Align-stage/</guid><description>Adapt and Align Stage The next step is to assess the model&amp;rsquo;s performance and perform additional training if needed
[[notes/202308090519 Prompt Engineering|Prompt Engineering]] can sometimes be enough to get the model to perform well, start with trying in-context learning, using examples suited to the task and use case.</description></item><item><title/><link>https://quartz.devkraft.in/notes/20230810063546-Select-Stage/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/20230810063546-Select-Stage/</guid><description>Select Stage One of the first things we have to decide is whether we&amp;rsquo;re taking a [[notes/20230806171025 Foundational Models|Foundational Models]] off the shelf or pre-training our own model</description></item><item><title/><link>https://quartz.devkraft.in/notes/20230810063625-Scope-Stage/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/20230810063625-Scope-Stage/</guid><description>Scope Stage The most important step in the project is to define the scope as accurately and narrowly as possible</description></item><item><title>Generative AI - A Mathematical Perspective</title><link>https://quartz.devkraft.in/notes/20230630041814-Generative-AI-A-Mathematical-Perspective/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/20230630041814-Generative-AI-A-Mathematical-Perspective/</guid><description>Generative AI - A Mathematical Perspective Generative AI is modeling a [[notes/20230630021029 Joint Distribution|Joint Distribution]] because the distribution itself is the object of interest</description></item><item><title>Generative AI vs Discriminative AI - A mathematical perspective</title><link>https://quartz.devkraft.in/notes/20230630020526-Generative-AI-vs-Discriminative-AI-A-mathematical-perspective/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/20230630020526-Generative-AI-vs-Discriminative-AI-A-mathematical-perspective/</guid><description>Generative AI vs Discriminative AI - A Mathematical Perspective Discriminative AI is considered to be modeling a [[notes/20230630021015 Conditional Distribution|Conditional Distribution]] whereas Generative AI is considered to be modeling a [[notes/20230630021029 Joint Distribution|Joint Distribution]]</description></item><item><title>Modern Generative AI</title><link>https://quartz.devkraft.in/notes/20230702204625-Modern-Generative-AI/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/20230702204625-Modern-Generative-AI/</guid><description>Modern Generative AI [[notes/20230628030901 Generative AI|Generative AI]] is not a new tech, but the recent explosion in performance and interest can be attributed to advances made in the last 5 years</description></item><item><title>Normalization</title><link>https://quartz.devkraft.in/notes/20230703034916-Normalization/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/20230703034916-Normalization/</guid><description>Normalization Normalization, in the context of machine learning, refers to the process of standardizing or scaling data to a common range or distribution.</description></item><item><title>Positional Encoding</title><link>https://quartz.devkraft.in/notes/20230703031923-Positional-Encoding/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/20230703031923-Positional-Encoding/</guid><description>Positional Encoding Positional Encoding is used to provide information about the position of words within a sentence.
Consider the sentence: &amp;ldquo;The cat sat on the mat.</description></item><item><title>Word Embedding</title><link>https://quartz.devkraft.in/notes/20230703031649-Word-Embedding/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/20230703031649-Word-Embedding/</guid><description>Word Embedding Word Embeddings are vector representations of words that capture semantic and syntactic relationships between words
consider the following words and respective word embeddings in a 3-dimensional space:</description></item></channel></rss>