<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>rn on</title><link>https://quartz.devkraft.in/tags/rn/</link><description>Recent content in rn on</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="https://quartz.devkraft.in/tags/rn/index.xml" rel="self" type="application/rss+xml"/><item><title>Generative AI - A Mathematical Perspective</title><link>https://quartz.devkraft.in/notes/20230630041814-Generative-AI-A-Mathematical-Perspective/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/20230630041814-Generative-AI-A-Mathematical-Perspective/</guid><description>Generative AI - A Mathematical Perspective Generative AI is modeling a [[notes/20230630021029 Joint Distribution|Joint Distribution]] because the distribution itself is the object of interest</description></item><item><title>Generative AI vs Discriminative AI - A mathematical perspective</title><link>https://quartz.devkraft.in/notes/20230630020526-Generative-AI-vs-Discriminative-AI-A-mathematical-perspective/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/20230630020526-Generative-AI-vs-Discriminative-AI-A-mathematical-perspective/</guid><description>Generative AI vs Discriminative AI - A Mathematical Perspective Discriminative AI is considered to be modeling a [[notes/20230630021015 Conditional Distribution|Conditional Distribution]] whereas Generative AI is considered to be modeling a [[notes/20230630021029 Joint Distribution|Joint Distribution]]</description></item><item><title>Modern Generative AI</title><link>https://quartz.devkraft.in/notes/20230702204625-Modern-Generative-AI/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/20230702204625-Modern-Generative-AI/</guid><description>Modern Generative AI [[notes/20230628030901 Generative AI|Generative AI]] is not a new tech, but the recent explosion in performance and interest can be attributed to advances made in the last 5 years</description></item><item><title>Normalization</title><link>https://quartz.devkraft.in/notes/20230703034916-Normalization/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/20230703034916-Normalization/</guid><description>Normalization Normalization, in the context of machine learning, refers to the process of standardizing or scaling data to a common range or distribution.</description></item><item><title>Positional Encoding</title><link>https://quartz.devkraft.in/notes/20230703031923-Positional-Encoding/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/20230703031923-Positional-Encoding/</guid><description>Positional Encoding Positional Encoding is used to provide information about the position of words within a sentence.
Consider the sentence: &amp;ldquo;The cat sat on the mat.</description></item><item><title>Word Embedding</title><link>https://quartz.devkraft.in/notes/20230703031649-Word-Embedding/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://quartz.devkraft.in/notes/20230703031649-Word-Embedding/</guid><description>Word Embedding Word Embeddings are vector representations of words that capture semantic and syntactic relationships between words
consider the following words and respective word embeddings in a 3-dimensional space:</description></item></channel></rss>