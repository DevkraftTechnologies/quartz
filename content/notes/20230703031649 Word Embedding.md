---
created: 2023-07-03 03:16
aliases: 
- Word Embedding
- Embedding
tags:
- rn
cssclasses:
- 
publish:
---

%% 
tags: 
%%

%%internal
parent:: [[]]
child:: [[]]
related:: [[]]
%%

%%external
- []()
%%

# Word Embedding

Word Embeddings are **vector representations of words** that **capture semantic and syntactic relationships between words**
  
consider the following words and respective word embeddings in a 3-dimensional space:
  
- "cat": `[0.2, 0.4, 0.1]`
- "dog": `[0.6, -0.3, 0.5]`
- "car": `[0.8, 0.2, -0.6]`
- "bike": `[0.7, -0.1, -0.4]`

In this example, each word is represented by a vector of 3 values, which can be interpreted as the word's features or characteristics. 

The values in the embeddings are learned through training algorithms such as Word2Vec or GloVe, which aim to capture the meaning and context of words based on their co-occurrence patterns in a large corpus of text

word embeddings allow the model to capture relationships between words. For instance, in this example, "cat" and "dog" have similar embeddings with positive values in the first dimension, indicating that these 2 words share some semantic similarity. 

on the other hand, "car" and "bike" have negative values in the third dimension, suggesting a potential contrast in their meanings.

<mark style="background: #BBFABBA6;">word embeddings allow models to represent words as continuous vectors</mark> and l<mark style="background: #BBFABBA6;">everage the geometric properties of these vectors</mark> to perform tasks like <mark style="background: #FF5582A6;">semantic similarity</mark>, <mark style="background: #FF5582A6;">word analogy</mark>, or <mark style="background: #FF5582A6;">text classification</mark>