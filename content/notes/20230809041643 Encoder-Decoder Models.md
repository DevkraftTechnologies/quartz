---
created: 2023-08-09 04:16
aliases: 
- Encoder-Decoder Models
tags:
- rn
cssclasses:
- 
publish:
dg-publish: false
---

<!-- 
tags: 
-->

<!--internal
parent:: [[202308090324 Generating Text with Transformers]]
child:: [[]]
related:: [[]]
-->

<!--external
- [ ] []()
-->

# Encoder-Decoder Models

- Encoder-decoder models (like BART) perform well on sequence-to-sequence tasks such as translation, where the input sequence and the output sequence can be different lengths. 
- You can also scale and train this type of model to perform general text generation tasks. 
- Examples of encoder-decoder models include BART as opposed to BERT and T5, the model that you'll use in the labs in this course. 