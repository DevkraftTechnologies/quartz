---
title: "GPT-3"
created: 2023-06-28 03:32
aliases: 
- GPT=3
- GPT3
tags:
- TODO
cssclasses:
- 
publish:
---

<!-- 
tags: 
-->

<!--internal
parent:: [[notes/20230628031147 Generative Pretrained Transformer (GPT)|GPT]]
child:: [[]]
related:: [[]]
-->

<!--external
- [ ] [How GPT3 Works - Visualizations and Animations](https://jalammar.github.io/how-gpt3-works-visualizations-animations/)
-->

# GPT-3

The architecture is a transformer decoder model

[[notes/20230628031147 Generative Pretrained Transformer (GPT)|GPT]]-3 works is novel due to it's huge scale. GPT-3 is MASSIVE. It encodes what it learns from training in 175 billion numbers (called parameters). 

These numbers are used to calculate which token to generate at each run.

The untrained model starts with random parameters. Training finds values that lead to better predictions

GPT-3 generates output one token at a time


