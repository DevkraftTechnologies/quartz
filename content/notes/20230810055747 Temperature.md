---
created: 2023-08-10 05:57
aliases: 
- Temperature
tags:
- rn
cssclasses:
- 
publish:
dg-publish: false
---

<!-- 
tags: 
-->

<!--internal
parent:: [[202308100523 Generative Configuration]]
child:: [[]]
related:: [[]]
-->

<!--external
- [ ] []()
-->

# Temperature

The temperature parameter influences the shape of the probability distribution that the model calculates for the next token

The higher the temperature, the higher the randomness and the lower the temperature, the lower the randomness.

The temperature value is a scaling factor that's applied within the final softmax layer of the model that impacts the shape of the probability distribution of the next token. 

In contrast to the top-k and top-p parameters, <mark style="background: #D2B3FFA6;">changing the temperature alters the predictions that the model will make. </mark>

for a low value of temperature (<1), the resulting probability distribution from the softmax layer is more strongly peaked with the probability being concentrated in a smaller number of words

![[notes/images/Screenshot 2023-08-10 at 5.54.12 AM.png]]

The model will select from this distribution using random sampling as such resulting text will be less random, closely following the most likely word sequences that the model learned during training

for a high value of temperature (>1), then the model will calculate a broader flatter probability distribution for the next token

![[notes/images/Screenshot 2023-08-10 at 5.55.02 AM.png]]

This leads the model to generate text with a higher degree of randomness and more variability in the output. It can help you generate text that sounds more creative

If temperature value is equal to one, softmax function will behave as default and the unaltered probability distribution will be used. 

