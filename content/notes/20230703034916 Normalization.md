---
created: 2023-07-03 03:49
aliases: 
- Normalization
tags:
- rn
cssclasses:
- 
publish:
---

<!-- 
tags: 
-->

<!--internal
parent:: [[]]
child:: [[]]
related:: [[]]
-->

<!--external
- []()
-->

# Normalization

Normalization, in the context of machine learning, refers to the process of standardizing or scaling data to a common range or distribution. 

It is often applied to features or variables in a dataset to ensure that the data have similar scales or magnitudes

Normalization helps to avoid biases or undue influences that might arise from features with different scales. 

The algorithm can compare and weigh the contributions of different features without a particular feature dominating the learning process based on its scale.

There are various methods of normalization, but most commonly used techniques are

1.  Min-Max Scaling (Normalization): This method scales the values of a feature to a fixed range, typically between 0 and 1. It is achieved by subtracting the minimum value of the feature and dividing it by the difference between the maximum and minimum values. The formula for min-max scaling is: normalized\_value = (value - min\_value) / (max\_value - min\_value)
    
2.  Z-Score Standardization: This method transforms the values of a feature to have a mean of 0 and a standard deviation of 1. It is achieved by subtracting the mean of the feature and dividing it by the standard deviation. The formula for z-score standardization is: standardized\_value = (value - mean) / standard\_deviation

**Normalization should be applied to numerical features, not categorical features**

It is important to note that **normalization is not always necessary or beneficial** for every machine learning algorithm. 

some algorithms like **Decision Trees or Random Forests, are not affected by feature scales** unlike others, like Logistic Regression or Neural Networks
