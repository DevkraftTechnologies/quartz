---
created: 2023-08-10 06:35
aliases: 
- Select Stage
tags:
- rn
cssclasses:
- 
publish:
dg-publish: false
---

<!-- 
tags: 
-->

<!--internal
parent:: [[202308061649 Generative AI project Lifecycle]]
child:: [[]]
related:: [[]]
-->

<!--external
- [ ] []()
-->

# Select Stage

One of the first things we have to decide is whether we're taking a [[notes/20230806171025 Foundational Models|Foundational Models]] off the shelf or [[notes/202308261520 Pretraining Large Language Models|Pre-training]] our own model

There are specific circumstances where training a model from scratch might be advantageous

In general, the process of developing application involves using an existing foundation model.

> when do you need a giant model, 100 billion (or bigger) parameters
>
> and when can a 1-30 billion parameter model or
>
> or even sub 1 billion parameter model made to work for a specific application?

>Â we can use quite small models and still get quite a lot of capability out of them

The developers of frameworks for building Generative AI applications like **Hugging Face** and PyTorch, have curated hubs where it is possible can browse open source models. 

These frameworks also provide **Model Cards** that describe important details including the 
- best use cases for each model, 
- how it was trained, 
- and known limitations

The exact model will depend on the details of the task

The variance of the [[notes/20230703011428 Transformers|Transformer]] model architecture are suited to different language tasks because of differences in how the models are trained. 
