---
title: "Transformers"
created: 2023-07-03 01:14
aliases: 
- Transformers
- Transformer
tags:
- TODO
cssclasses:
- 
publish:
---

<!-- 
tags: 
-->

<!--internal
parent:: [[]]
child:: [[]]
related:: [[]]
-->

<!--external
- [ ] [Assembly AI](https://www.youtube.com/@AssemblyAI)'s [Transformers for Beginners](https://www.youtube.com/watch?v=_UVfwBqcnbM)
-->

# Transformers

## Overview

- Transformers were introduced in [Attention Is All You Need](https://arxiv.org/abs/1706.03762) [[notes/202308061603 Research Papers|Research Paper]]
- The power of the transformer architecture lies in its ability to learn the relevance and context not just of each word next to its neighbor ![[Screenshot 2023-08-07 at 5.12.02 AM.png]] but to every other word in the sentence ![[Screenshot 2023-08-07 at 5.13.21 AM.png]]
- It is able to set attention weights to those relationships so that the model learns the relevance of each word to each other word no matter where it is in the input ![[Screenshot 2023-08-07 at 5.06.37 AM.png]]!
- This diagram above is called an attention map and is useful to illustrate the attention weights between each word and every other word ![[Screenshot 2023-08-07 at 5.09.34 AM.png]]
- The word book is strongly connected with or paying attention to the word teacher and the word student. This is called self-attention
- \Transformers use [[notes/20230703013343 Attention|Attention]] mechanisms instead of recurrence [[notes/20230703012211 Recurrent Neural Networks (RNNs)|Recurrent Neural Networks (RNNs)]] to remember information, making them faster and **parallelizable**

![[notes/20230807051721 Transformer Architecture|Transformer Architecture]]