---
title: "Transformers"
created: 2023-07-03 01:14
aliases: 
- Transformers
- Transformer
tags:
- TODO
cssclasses:
- 
publish:
---

<!-- 
tags: 
-->

<!--internal
parent:: [[]]
child:: [[]]
related:: [[]]
-->

<!--external
- [ ] [Assembly AI](https://www.youtube.com/@AssemblyAI)'s [Transformers for Beginners](https://www.youtube.com/watch?v=_UVfwBqcnbM)
-->

# Transformers

![[notes/images/Screenshot 2023-07-03 at 1.28.32 AM.png|250]]

- Transformers use [[notes/20230703013343 Attention|Attention]] mechanisms instead of  [[notes/20230703012211 Recurrent Neural Networks (RNNs)|Recurrent Neural Networks (RNNs)]] to remember information, making them faster and parallelizable.
- The architecture of a transformer includes six <mark style="background: #FF5582A6;">encoders</mark> and six <mark style="background: #FF5582A6;">decoders</mark>, 
- each encoder has a multi-head self-attention and feed-forward neural network
- each decoder has 1 multi-head self-attention, 1 masked multi-head attention and 1 feed-forward neural network
- Inputs are [[notes/20230703031649 Word Embedding|Embedded]] and given [[notes/20230703031923 Positional Encoding|Positional Encoding]]s
- parallelization is performed with feeding all words of the sentence through the multi-head self-attention layer
- The words are then passed through the feed-forward neural network. **In each of the 6 encoders, neural networks are different**
- In between each sub-layer, there are Add and Normalized layer to [[notes/20230703034916 Normalization|Normalize]] the output out of the sub-layer
- The normalization technique used here is called **Layer Normalization**
- The output is transformed into a probability distribution over the vocabulary using a linear layer and <mark style="background: #FF5582A6;">softmax</mark>