---
created: 2023-08-26 17:00
aliases: 
- Data for Training Large Language Models
tags:
- rn
cssclasses:
- 
publish:
dg-publish: false
---

<!-- 
tags: 
-->

<!--internal
parent:: [[202308261520 Pretraining Large Language Models]]
child:: [[]]
related:: [[]]
-->

<!--external
- [ ] []()
-->

# Data for Training Large Language Models

We need to process the data to increase quality, address bias, and remove other harmful content esp. if training data is scraped from public sites on the internet

As such, A small percentage (1-3%() of tokens is used for pre-training. This has to be taken into account when estimating how much data is needed to train a model from scratch